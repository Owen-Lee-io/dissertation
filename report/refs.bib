@Article{skinDataset,
author="Rajen Bhatt, Abhinav Dhall",
title="Skin Segmentation Dataset", 
url="https://archive.ics.uci.edu/ml/datasets/skin+segmentation#",
journal="UCI Machine Learning Repository"
}
@Article{vanderKooij2019,
author={van der Kooij, Koen M.
and Naber, Marnix},
title={An open-source remote heart rate imaging method with practical apparatus and algorithms},
journal={Behavior Research Methods},
year={2019},
volume={51},
number={5},
pages={2106-2119},
abstract={Recent developments in computer science and digital image processing have enabled the extraction of an individual's heart pulsations from pixel changes in recorded video images of human skin surfaces. This method is termed remote photoplethysmography (rPPG) and can be achieved with consumer-level cameras (e.g., a webcam or mobile camera). The goal of the present publication is two-fold. First, we aim to organize future rPPG software developments in a tractable and nontechnical manner, such that the public gains access to a basic open-source rPPG code, comes to understand its utility, and can follow its most recent progressions. The second goal is to investigate rPPG's accuracy in detecting heart rates from the skin surfaces of several body parts after physical exercise and under ambient lighting conditions with a consumer-level camera. We report that rPPG is highly accurate when the camera is aimed at facial skin tissue, but that the heart rate recordings from wrist regions are less reliable, and recordings from the calves are unreliable. Facial rPPG remained accurate despite the high heart rates after exercise. The proposed research procedures and the experimental findings provide guidelines for future studies on rPPG.},
issn={1554-3528},
doi={10.3758/s13428-019-01256-8},
url={https://doi.org/10.3758/s13428-019-01256-8}
}
@workshop{LucasKanade,
author = {Bruce D. Lucas and Takeo Kanade},
title = {An Iterative Image Registration Technique with an Application to Stereo Vision (DARPA)},
booktitle = {Proceedings of the 1981 DARPA Image Understanding Workshop},
year = {1981},
month = {April},
pages = {121-130},
} 
@Article{Mahnob,
author = {M. Soleymani, J. Lichtenauer, T. Pun and M. Pantic.},
title = {"A multimodal database for affect recognition and implicit tagging"},
year = {2012},
month = {April},
pages = {42-55},
journal={IEEE Transactions on Affective Computing. 3},
}
@Article{Verkruysse2008,
author={Verkruysse, Wim
and Svaasand, Lars O.
and Nelson, J. Stuart},
title={Remote plethysmographic imaging using ambient light},
journal={Optics express},
year={2008},
month={Dec},
day={22},
volume={16},
number={26},
pages={21434-21445},
keywords={Diagnostic Imaging/*methods},
keywords={Equipment Design},
keywords={Exercise},
keywords={Heart Rate},
keywords={Hemoglobins/chemistry},
keywords={Humans},
keywords={*Light},
keywords={Male},
keywords={Monitoring, Physiologic/*methods},
keywords={Oxygen/chemistry},
keywords={Plethysmography/*instrumentation/*methods},
keywords={Respiration},
keywords={Time Factors},
keywords={Video Recording},
abstract={Plethysmographic signals were measured remotely (> 1m) using ambient light and a simple consumer level digital camera in movie mode. Heart and respiration rates could be quantified up to several harmonics. Although the green channel featuring the strongest plethysmographic signal, corresponding to an absorption peak by (oxy-) hemoglobin, the red and blue channels also contained plethysmographic information. The results show that ambient light photo-plethysmography may be useful for medical purposes such as characterization of vascular skin lesions (e.g., port wine stains) and remote sensing of vital signs (e.g., heart and respiration rates) for triage or sports purposes.},
note={19104573[pmid]},
note={PMC2717852[pmcid]},
note={175396[PII]},
issn={1094-4087},
doi={10.1364/oe.16.021434},
url={https://pubmed.ncbi.nlm.nih.gov/19104573}
}
@InProceedings{Balakrishnan_2013_CVPR,
author = {Balakrishnan, Guha and Durand, Fredo and Guttag, John},
title = {Detecting Pulse from Head Motions in Video},
booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2013}
} 
@article{poh2010non,
  title={Non-contact, automated cardiac pulse measurements using video imaging and blind source separation.},
  author={Poh, Ming-Zher and McDuff, Daniel J and Picard, Rosalind W},
  journal={Optics express},
  volume={18},
  number={10},
  pages={10762--10774},
  year={2010},
  publisher={Optical Society of America}
}
@Article{Viola2004,
author={Viola, Paul
and Jones, Michael J.},
title={Robust Real-Time Face Detection},
journal={International Journal of Computer Vision},
year={2004},
volume={57},
number={2},
pages={137-154},
abstract={This paper describes a face detection framework that is capable of processing images extremely rapidly while achieving high detection rates. There are three key contributions. The first is the introduction of a new image representation called the "Integral Image" which allows the features used by our detector to be computed very quickly. The second is a simple and efficient classifier which is built using the AdaBoost learning algorithm (Freund and Schapire, 1995) to select a small number of critical visual features from a very large set of potential features. The third contribution is a method for combining classifiers in a "cascade" which allows background regions of the image to be quickly discarded while spending more computation on promising face-like regions. A set of experiments in the domain of face detection is presented. The system yields face detection performance comparable to the best previous systems (Sung and Poggio, 1998; Rowley et al., 1998; Schneiderman and Kanade, 2000; Roth et al., 2000). Implemented on a conventional desktop, face detection proceeds at 15 frames per second.},
issn={1573-1405},
doi={10.1023/B:VISI.0000013087.49260.fb},
url={https://doi.org/10.1023/B:VISI.0000013087.49260.fb}
}
@ARTICLE{655647,  author={H. A. {Rowley} and S. {Baluja} and T. {Kanade}},  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},  title={Neural network-based face detection},   year={1998},  volume={20},  number={1},  pages={23-38},}
@INPROCEEDINGS{v-j-vs-ann,  author={M. {Da'san} and A. {Alqudah} and O. {Debeir}},  booktitle={2015 International Conference on Information and Communication Technology Research (ICTRC)},  title={Face detection using Viola and Jones method and neural networks},   year={2015},  volume={},  number={},  pages={40-43},}
@Inbook{Fleet2006,
author="Fleet, D.
and Weiss, Y.",
editor="Paragios, Nikos
and Chen, Yunmei
and Faugeras, Olivier",
title="Optical Flow Estimation",
bookTitle="Handbook of Mathematical Models in Computer Vision",
year="2006",
publisher="Springer US",
address="Boston, MA",
pages="237--257",
abstract="This chapter provides a tutorial introduction to gradient-based optical flow estimation. We discuss least-squares and robust estimators, iterative coarse-to-fine refinement, different forms of parametric motion models, different conservation assumptions, probabilistic formulations, and robust mixture models.",
isbn="978-0-387-28831-4",
doi="10.1007/0-387-28831-7_15",
url="https://doi.org/10.1007/0-387-28831-7_15"
}
@INPROCEEDINGS{
ShiTomasi,  author={ {Jianbo Shi} and  {Tomasi}},  booktitle={1994 Proceedings of IEEE Conference on Computer Vision and Pattern Recognition},  title={Good features to track},   year={1994},  volume={},  number={},  pages={593-600},
}
@article{Cherry,
author = {Cherry,E. Colin },
title = {Some Experiments on the Recognition of Speech, with One and with Two Ears},
journal = {The Journal of the Acoustical Society of America},
volume = {25},
number = {5},
pages = {975-979},
year = {1953},
doi = {10.1121/1.1907229},

URL = { 
        https://doi.org/10.1121/1.1907229
    
},
eprint = { 
        https://doi.org/10.1121/1.1907229
    
}

}

@article{novel,
	Abstract = {In this paper we present a novel health monitoring method by estimating the heart rate and respiratory rate using an RGB camera. The heart rate and the respiratory rate are estimated from the photoplethysmography (PPG) and the respiratory motion. The method mainly operates by using the green spectrum of the RGB camera to generate a multivariate PPG signal to perform multivariate de-noising on the video signal to extract the resultant PPG signal. A periodicity based voting scheme (PVS) was used to measure the heart rate and respiratory rate from the estimated PPG signal. We evaluated our proposed method with a state of the art heart rate measuring method for two scenarios using the MAHNOB-HCI database and a self collected naturalistic environment database. The methods were furthermore evaluated for various scenarios at naturalistic environments such as a motion variance session and a skin tone variance session. Our proposed method operated robustly during the experiments and outperformed the state of the art heart rate measuring methods by compensating the effects of the naturalistic environment.},
	An = {29188085},
	Author = {Hassan, M A and Malik, A S and Fofi, D and Saad, N and Meriaudeau, F},
	Date = {2017/10/04},
	Date-Added = {2020-04-04 17:13:45 +0100},
	Date-Modified = {2020-04-04 17:13:45 +0100},
	Db = {PubMed},
	Doi = {10.1364/BOE.8.004838},
	Isbn = {2156-7085; 2156-7085},
	J2 = {Biomed Opt Express},
	Journal = {Biomedical optics express},
	Keywords = {(330.0330) Vision, color, and visual optics; (330.7326) Visual optics, modeling},
	L2 = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5695935/},
	La = {eng},
	Month = {10},
	Number = {11},
	Pages = {4838--4854},
	Publisher = {Optical Society of America},
	Title = {Novel health monitoring method using an RGB camera},
	Ty = {JOUR},
	U1 = {29188085{$[$}pmid{$]$}},
	U2 = {PMC5695935{$[$}pmcid{$]$}},
	U4 = {292965{$[$}PII{$]$}},
	Url = {https://pubmed.ncbi.nlm.nih.gov/29188085},
	Volume = {8},
	Year = {2017},
	Bdsk-Url-1 = {https://pubmed.ncbi.nlm.nih.gov/29188085},
	Bdsk-Url-2 = {https://doi.org/10.1364/BOE.8.004838}}



@INPROCEEDINGS{mahnob-example,  author={X. {Li} and J. {Chen} and G. {Zhao} and M. {Pietikäinen}},  booktitle={2014 IEEE Conference on Computer Vision and Pattern Recognition},  title={Remote Heart Rate Measurement from Face Videos under Realistic Situations},   year={2014},  volume={},  number={},  pages={4264-4271},}

@INPROCEEDINGS{mahnob-example-2,  author={X. {Niu} and H. {Han} and S. {Shan} and X. {Chen}},  booktitle={2017 IEEE International Joint Conference on Biometrics (IJCB)},  title={Continuous heart rate measurement from face: A robust rPPG approach with distribution learning},   year={2017},  volume={},  number={},  pages={642-650},}

@misc{Python,
  title = {{Python} programming language},
  howpublished = {\url{python.org}},
  note = {Accessed: 2019-10}
}
@misc{Kotlin,
  title = {{Kotlin} programming language},
  howpublished = {\url{kotlinlang.org}},
  note = {Accessed: 2019-10}
}
@misc{OpenCV,
  title = {{OpenCV} library},
  howpublished = {\url{opencv.org}},
  note = {Accessed: 2019-10}
}
@misc{MobileVision,
  title = {{Mobile Vision} API},
  howpublished = {\url{https://developers.google.com/vision}},
  note = {Accessed: 2019-10}
}
@misc{NumPy,
  title = {{NumPy} library},
  howpublished = {\url{numpy.org}},
  note = {Accessed: 2019-10}
}
@misc{sklearn,
  title = {{scikit-learn} library},
  howpublished = {\url{scikit-learning.org}},
  note = {Accessed: 2019-10}
}
@InProceedings{Chen_2018_ECCV,
author = {Chen, Weixuan and McDuff, Daniel},
title = {DeepPhys: Video-Based Physiological Measurement Using Convolutional Attention Networks},
booktitle = {The European Conference on Computer Vision (ECCV)},
month = {September},
year = {2018}
} 

@ARTICLE{comparative,
  
AUTHOR={Wang, Chen and Pun, Thierry and Chanel, Guillaume},   
	 
TITLE={A Comparative Survey of Methods for Remote Heart Rate Detection From Frontal Face Videos},      
	
JOURNAL={Frontiers in Bioengineering and Biotechnology},      
	
VOLUME={6},      

PAGES={33},     
	
YEAR={2018},      
	  
URL={https://www.frontiersin.org/article/10.3389/fbioe.2018.00033},       
	
DOI={10.3389/fbioe.2018.00033},      
	
ISSN={2296-4185},   
   
ABSTRACT={Remotely measuring physiological activity can provide substantial benefits for both the medical and the affective computing applications. Recent research has proposed different methodologies for the unobtrusive detection of heart rate (HR) using human face recordings. These methods are based on subtle color changes or motions of the face due to cardiovascular activities, which are invisible to human eyes but can be captured by digital cameras. Several approaches have been proposed such as signal processing and machine learning. However, these methods are compared with different datasets, and there is consequently no consensus on method performance. In this article, we describe and evaluate several methods defined in literature, from 2008 until present day, for the remote detection of HR using human face recordings. The general HR processing pipeline is divided into three stages: face video processing, face blood volume pulse (BVP) signal extraction, and HR computation. Approaches presented in the paper are classified and grouped according to each stage. At each stage, algorithms are analyzed and compared based on their performance using the public database MAHNOB-HCI. Results found in this article are limited on MAHNOB-HCI dataset. Results show that extracted face skin area contains more BVP information. Blind source separation and peak detection methods are more robust with head motions for estimating HR.}
}
@INPROCEEDINGS{osman,  author={A. {Osman} and J. {Turcot} and R. E. {Kaliouby}},  booktitle={2015 11th IEEE International Conference and Workshops on Automatic Face and Gesture Recognition (FG)},  title={Supervised learning approach to remote heart rate estimation from facial videos},   year={2015},  volume={1},  number={},  pages={1-6},}
@article{poh,
  title={Advancements in noncontact, multiparameter physiological measurements using a webcam},
  author={Poh, Ming-Zher and McDuff, Daniel J and Picard, Rosalind W},
  journal={IEEE transactions on biomedical engineering},
  volume={58},
  number={1},
  pages={7--11},
  year={2010},
  publisher={IEEE}
}
@inproceedings{li,
  title={Remote heart rate measurement from face videos under realistic situations},
  author={Li, Xiaobai and Chen, Jie and Zhao, Guoying and Pietikainen, Matti},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={4264--4271},
  year={2014}
}
@inproceedings{spec1,
  title={Real-time specular highlight removal using bilateral filtering},
  author={Yang, Qingxiong and Wang, Shengnan and Ahuja, Narendra},
  booktitle={European conference on computer vision},
  pages={87--100},
  year={2010},
  organization={Springer}
}

@article{spec2,
  title={Efficient and robust specular highlight removal},
  author={Yang, Qingxiong and Tang, Jinhui and Ahuja, Narendra},
  journal={IEEE transactions on pattern analysis and machine intelligence},
  volume={37},
  number={6},
  pages={1304--1311},
  year={2014},
  publisher={IEEE}
}

@inproceedings{spec3,
  title={Specular highlight removal in facial images},
  author={Li, Chen and Lin, Stephen and Zhou, Kun and Ikeuchi, Katsushi},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={3107--3116},
  year={2017}
}
@article{souza2019heart,
  title={Heart Rate Estimation During Physical Exercise Using Wrist-Type Ppg Sensors},
  author={Souza, Thiago Toledo},
  year={2019},
  publisher={University of Windsor}
}
@article{welch-t-test,
    author = {WELCH, B. L.},
    title = "{The generalization of ‘student's’ problem when several different population varlances are involved}",
    journal = {Biometrika},
    volume = {34},
    number = {1-2},
    pages = {28-35},
    year = {1947},
    month = {01},
    issn = {0006-3444},
    doi = {10.1093/biomet/34.1-2.28},
    url = {https://doi.org/10.1093/biomet/34.1-2.28},
    eprint = {https://academic.oup.com/biomet/article-pdf/34/1-2/28/553093/34-1-2-28.pdf},
}

@article{superior-welch,
    author = {Ruxton, Graeme D.},
    title = "{The unequal variance t-test is an underused alternative to Student's t-test and the Mann–Whitney U test}",
    journal = {Behavioral Ecology},
    volume = {17},
    number = {4},
    pages = {688-690},
    year = {2006},
    month = {05},
    abstract = "{Often in the study of behavioral ecology, and more widely in science, we require to statistically test whether the central tendencies (mean or median) of 2 groups are different from each other on the basis of samples of the 2 groups. In surveying recent issues of Behavioral Ecology (Volume 16, issues 1–5), I found that, of the 130 papers, 33 (25\\%) used at least one statistical comparison of this sort. Three different tests were used to make this comparison: Student's t-test (67 occasions; 26 papers), Mann–Whitney U test (43 occasions; 21 papers), and the t-test for unequal variances (9 occasions; 4 papers). My aim in this forum article is to argue for the greater use of the last of these tests. The numbers just related suggest that this test is not commonly used. In my survey, I was able to identify tests described simply as “t-tests” with confidence as either a Student's t-test or an unequal variance t-test because the calculation of degrees of freedom from the 2 sample sizes is different for the 2 tests (see below). Hence, the neglect of the unequal variance t-test illustrated above is a real phenomenon and can be explained in several (nonexclusive ways) ways:Argument 4 relies on the central limit theorem and would require a combined sample size of at least 30 (Sokal and Rohlf 1987, p. 107); however, in my survey, the majority (47 out of 61) of tests for which sample sizes were provided had a combined sample size below 30. The fallacy of argument 3 has been demonstrated previously on several occasions (e.g., Kasuya 2001; Neuhauser 2002). To explore argument 1 further, imagine that we have 2 sample groups (labeled “1” and “2,” with means [μ1 and μ2], variances [\\batchmode \\documentclass[fleqn,10pt,legalpaper]\\{article\\} \\usepackage\\{amssymb\\} \\usepackage\\{amsfonts\\} \\usepackage\\{amsmath\\} \\pagestyle\\{empty\\} \\begin\\{document\\} \\(s\_\\{1\\}^\\{2\\}\\) \\end\\{document\\} and \\batchmode \\documentclass[fleqn,10pt,legalpaper]\\{article\\} \\usepackage\\{amssymb\\} \\usepackage\\{amsfonts\\} \\usepackage\\{amsmath\\} \\pagestyle\\{empty\\} \\begin\\{document\\} \\(s\_\\{2\\}^\\{2\\}\\) \\end\\{document\\}], and sample sizes [N1 and N2]). For the unpaired Student's t-test, the t statistic is calculated as(1)\\batchmode \\documentclass[fleqn,10pt,legalpaper]\\{article\\} \\usepackage\\{amssymb\\} \\usepackage\\{amsfonts\\} \\usepackage\\{amsmath\\} \\pagestyle\\{empty\\} \\begin\\{document\\} \\[t\\{=\\}\\frac\\{\\mathrm\\{\\{\\mu\\}\\}\_\\{1\\}\\{-\\}\\mathrm\\{\\{\\mu\\}\\}\_\\{2\\}\\}\\{s\_\\{p\\}^\\{2\\}\\sqrt\\{\\left(\\frac\\{1\\}\\{n\_\\{1\\}\\}\\{+\\}\\frac\\{1\\}\\{n\_\\{2\\}\\}\\right)\\}\\},\\] \\end\\{document\\}where the pooled variance \\batchmode \\documentclass[fleqn,10pt,legalpaper]\\{article\\} \\usepackage\\{amssymb\\} \\usepackage\\{amsfonts\\} \\usepackage\\{amsmath\\} \\pagestyle\\{empty\\} \\begin\\{document\\} \\(s\_\\{p\\}^\\{2\\}\\) \\end\\{document\\} is given by(2)\\batchmode \\documentclass[fleqn,10pt,legalpaper]\\{article\\} \\usepackage\\{amssymb\\} \\usepackage\\{amsfonts\\} \\usepackage\\{amsmath\\} \\pagestyle\\{empty\\} \\begin\\{document\\} \\[s\_\\{p\\}^\\{2\\}\\{=\\}\\frac\\{(n\_\\{1\\}\\{-\\}1)s\_\\{1\\}^\\{2\\}\\{+\\}(n\_\\{2\\}\\{-\\}1)s\_\\{2\\}^\\{2\\}\\}\\{n\_\\{1\\}\\{+\\}n\_\\{2\\}\\{-\\}2\\}.\\] \\end\\{document\\}The variances of the 2 samples are pooled in order to achieve the best estimate of the (assumed equal) variances of the 2 populations. Hence, we can see the need for the underlying assumption of equal population variances in this test. The Student's t-test performs badly when these variances are actually unequal, both in terms of Type I and Type II errors (Zar 1996). Figure 1 suggests that unequal sample variances are common in behavioral ecology. Although it is true that unequal variances are less problematic if sample sizes are similar, in practice, we often have quite unequal sample sizes (Figure 2). Hence, I suggest that the Student's t-test is frequently used in behavioral ecology when one of its important underlying assumptions is violated, and consequently, its performance is unreliable. }",
    issn = {1045-2249},
    doi = {10.1093/beheco/ark016},
    url = {https://doi.org/10.1093/beheco/ark016},
    eprint = {https://academic.oup.com/beheco/article-pdf/17/4/688/17275561/ark016.pdf},
}


@Article{originalPaper,
author="Verkruysse W, Svaasand LO, Nelson JS",
title="Remote plethysmographic imaging using ambient light",
year="2009",
month="Jul",
day="29",
abstract="Plethysmographic signals were measured remotely (> 1m) using ambient light and a simple consumer level digital camera in movie mode. Heart and respiration rates could be quantified up to several harmonics. Although the green channel featuring the strongest plethysmographic signal, corresponding to an absorption peak by (oxy-) hemoglobin, the red and blue channels also contained plethysmographic information. The results show that ambient light photo-plethysmography may be useful for medical purposes such as characterization of vascular skin lesions (e.g., port wine stains) and remote sensing of vital signs (e.g., heart and respiration rates) for triage or sports purposes.",
issn="1094-4087",
doi="10.1364/oe.16.021434",
url="https://doi.org/10.1364/oe.16.021434"
}

@Article{Peng2014,
author={Peng, Fulai
and Zhang, Zhengbo
and Gou, Xiaoming
and Liu, Hongyun
and Wang, Weidong},
title={Motion artifact removal from photoplethysmographic signals by combining temporally constrained independent component analysis and adaptive filter},
journal={BioMedical Engineering OnLine},
year={2014},
volume={13},
number={1},
pages={50},
abstract={The calculation of arterial oxygen saturation (SpO2) relies heavily on the amplitude information of the high-quality photoplethysmographic (PPG) signals, which could be contaminated by motion artifacts (MA) during monitoring.},
issn={1475-925X},
doi={10.1186/1475-925X-13-50},
url={https://doi.org/10.1186/1475-925X-13-50}
}


@InProceedings{10.1007/978-3-030-01216-8_22,
author="Chen, Weixuan
and McDuff, Daniel",
editor="Ferrari, Vittorio
and Hebert, Martial
and Sminchisescu, Cristian
and Weiss, Yair",
title="DeepPhys: Video-Based Physiological Measurement Using Convolutional Attention Networks",
booktitle="Computer Vision -- ECCV 2018",
year="2018",
publisher="Springer International Publishing",
address="Cham",
pages="356--373",
abstract="Non-contact video-based physiological measurement has many applications in health care and human-computer interaction. Practical applications require measurements to be accurate even in the presence of large head rotations. We propose the first end-to-end system for video-based measurement of heart and breathing rate using a deep convolutional network. The system features a new motion representation based on a skin reflection model and a new attention mechanism using appearance information to guide motion estimation, both of which enable robust measurement under heterogeneous lighting and major motions. Our approach significantly outperforms all current state-of-the-art methods on both RGB and infrared video datasets. Furthermore, it allows spatial-temporal distributions of physiological signals to be visualized via the attention mechanism.",
isbn="978-3-030-01216-8"
}

@article{li2018differentiable,
  title={Differentiable programming for image processing and deep learning in Halide},
  author={Li, Tzu-Mao and Gharbi, Micha{\"e}l and Adams, Andrew and Durand, Fr{\'e}do and Ragan-Kelley, Jonathan},
  journal={ACM Transactions on Graphics (TOG)},
  volume={37},
  number={4},
  pages={139},
  year={2018},
  publisher={ACM}
}



