\section{Overview}
% In this chapter I will outline the implementation 
% GIVE STRUCTURE OF PROGRAM AND EXPLAIN WHY MOST WORK IS DEDICATED TO REGION
% SELECTION AND HEART RATE ISOLATION SPECIFICALLY
% THAT IS FACE DETECTION IS MOSTLY A SOLVED PROBLEM
% BACKGROUND ON HOW DESIGN IS TO MIMIC A PPG SENSOR
Abstractly, the program consists of three distinct tasks, each of which rely on the result from the previous. Together, forming a kind of pipeline:
\begin{itemize}
    \item \textbf{Face detection}: identify a face in each supplied camera frame
    \item \textbf{Region selection}: given a bounding box around a face, select some set of pixels to consider which are amenable to heart rate detection
    \item \textbf{Heart rate isolation}: given a time series of the mean colour of some region of the face, infer the heart rate
\end{itemize}
Face detection is largely a solved problem and thereby the project mostly concentrates on the latter two, for which, there is very much ongoing research.

\section{System design}
\label{section:system_design}
Each of these tasks occur at different rates and, thereby, have different performance constraints which must be upheld. Face detection and region selection operate on every frame received from the camera and so must run in real-time, or risk dropping streamed frames.
\\ \\
Heart rate isolation, however, is only executed after some adequate number of data points is received and is recomputed after a fixed time. Since none of the prior stages rely on the estimated heart rate, its execution time requirements are less stringent. This is exploited to perform relatively expensive analyses without slowing earlier stages. Crucially, this relies on the assumption that it can be run concurrently from the earlier stages.
% The system was designed with a focus on decomposition. 
Separating these tasks allows for this concurrency to be implemented safely.

\begin{figure}[H]
    % \includegraphics[width=\textwidth]{example-image-a}
    \centering
    \input{implementation/uml.tex}
   \caption{A UML sequence diagram showcasing the use of threading} 
\end{figure}
\noindent
The camera streams frames to the \textbf{FaceDetector}. The \textbf{RegionSelector} then takes the mean pixel colour of the region considered and adds this value to the dataset. Once enough values have been collected, as defined by the window size, the \textbf{HRIsolator} spawns a new thread, the \textbf{IsolationTask}, which attempts to infer the heart rate from the window of values. 
\\ \\
Although the region selection could also be executed in a separate thread, the associated setup costs are likely to have an adverse impact on real-time performance. However, one might wish to use more expensive region selection algorithms which cannot run in real-time. In these scenarios, the program could copy the frame and execute the selection in a separate thread from the main face detection loop. This is fundamentally a tradeoff between memory usage and execution time. Furthermore, since there are only a finite number of threads which can be spawned, quickly the limit might be reached without providing much additional computation time to the \textbf{RegionSelector}. As a result, the \textbf{FaceDetector} and \textbf{RegionSelector} operate sequentially in the same thread.
% \subsection{Asynchronous execution}
% TALK ABOUT CONCURRENCY RUNNING HR ISOLATION ASYNCHRONOUSLY
\section{Face detection}
% TALK ABOUT CRITICAL NOT TO DROP FRAMES
A face detector is expected to take a single frame and return a bounding rectangle within which a face is present. This could be extended to work on a stream of frames, naÃ¯vely, by simply repeated applying this face detector to each frame independently.

\subsection{Face tracking}
\label{section:face_tracking_impl}
Smartphone cameras can readily stream at high framerates, so it is unlikely that a face moves very much between a pair of consecutive frames. At sixty frames per second, frames are recorded only 0.017s apart. The position of the face in the previous frame, gives a strong indication of its subsequent position. Thus, there is opportunity for optimisation beyond simply applying a face detector to each frame individually. This is important since a speedup in this part of the pipeline provides opportunity for more expensive computation in subsequent stages which might improve accuracy. However, it is critical that any optimisations are resistant to motion. Using information from previous frames forms the distinction between face \textbf{\textit{detection}} and \textbf{\textit{tracking}}.

\subsubsection{Point tracking}
The key principle behind face tracking is to use information from previous frames to reduce the cost of subsequent face detections.
To that end, I implemented an optical-flow based algorithm that tracks points on the face rather than repeatedly calling the face detector. The Lucas-Kanade algorithm \cite{LucasKanade}, a particular variant of optical-flow, tracks a set of points between consecutive frames. Naturally, over time, these points will diverge from their true positions. This is because images contain large numbers of pixels with similar illumination, over time, these cannot be distinguished perfectly. When this occurs, the position of the face should be redetected. 
\\ \\
Knowing when the points have diverged is non-trivial, since the true location of the face is unknown. It is crucial for any implementation to act safely enough that the face is not lost track of, whilst simultaneously minimising the number of redetections.
There are two obvious approaches to combat this. The algorithm could redetect the face: 
\begin{itemize}
    \item periodically 
    \item when the tracked face changes in size significantly. 
\end{itemize}
The former wastes computation time for stationary videos where the redetections might be unnecessary. Simultaneously, the time between redetections must be short enough to deal with videos with significant movement. This static approach, therefore, was not considered. For this reason, the latter was implemented. We track the face and if the size of the box surrounding it changes significantly, indicating that our confidence in the tracked points has reduced, then we recompute the true position of the face.
\begin{figure}[H]
\begin{minted}{python}
   points = []
   last_detection = None
   def face_tracker(frame):
        if redetect or points is empty:
            face = face_detector(frame) 
            last_detection = face
            points = select_new_points(face)
        else:
            points = track_points(points)
            face = bounding_box(points)
        redetect = change_in_size(last_detection, face) > threshold
        previous_face = face
        return face
\end{minted}
\caption{A simplified pseudocode representation of an optical-flow based face tracker}
\end{figure} 
\paragraph{Impact of the rate of redetections}
It is important to reason about precisely when face tracking provides a performance boost. To understand this let us consider a sequence of frames which the face tracker has been applied to. The cost of tracking is compared with that of repeatedly detecting the face in each frame independently. Under the following notation:
\begin{itemize}
   \item $W$: number of consecutive frames considered 
   \item $R$: the total number of redetections by the face tracker
   \item $n$: size of each frame in pixels
   \item $p$: number of points tracked
   \item $f(n)$: cost of a face detection on a single frame of size $n$
   \item $s(p,f)$: cost of selecting $p$ points to track in a face of size $f$
   \item $g(p,n)$: cost of tracking $p$ points in a frame of size $n$
\end{itemize}
The cost of the repeated face detector is $Wf(n)$. Since, for each frame in the sequence, it detects the face independently and incurs a cost of $f(n)$.  
\\ \\
The point tracking approach instead has a cost of $Wg(p,n) + R(f(n) + s(p,f))$. For each redetection it calls the face detector and selects a new set of points. It also tracks the set of points for every frame, hence the term $Wg(p,n)$. In the worst case, $R=W$, so there is no saving in terms of computational complexity. Instead let us investigate for what values of $R$ there is a cost saving.
\begin{align*}
    Wf(n) &> Wg(p,n) + R[f(n) + s(p,f)] \\
    R &< \frac{W[f(n) - g(p,n)]}{f(n) + s(p,f)}\\
    \frac{R}{W} &< \frac{f(n)-g(p,n)}{f(n)+s(p,f)}
\end{align*}
Notice that $R/W$ represents the percentage of frames for which a redetection occurs. There is only a performance saving when this percentage falls below the value on the right hand side. Furthermore, this value itself is based on the relative costs of face detection and of selecting and tracking points. Implicitly, the algorithm assumes that the cost of tracking and selecting points is less than that of face detection, otherwise, this endeavour would be useless. This assumption is validated and the limit is evaluated experimentally in section \ref{section:face_tracking}. From this it is shown that even for videos with lots of movement, the value $R/W$ falls below this limit and the inequality is satisfied. 
% The algorithm above works by selecting some number of points on the face and tracking them between frames. If the points move such that the size of the bounding box increases above some threshold from the last face detection, then the face detector is called again. 
% \paragraph{Assumptions}
% This relies on the assumption that if the points lose track of the face, then the threshold is such that 
% talk about probability of a translation which is the main failure case under the assumption of constant size between frames
% the alternative assumption would be to assume a stationary center but comment that in practice points tend to move outwards since some points will 
\paragraph{Details}
To maximise the performance of the optical flow algorithm, selecting points randomly will not suffice. That is because, points which are particularly different to their neighbours, for example, are much easier to track. To achieve this, the Shi-Tomasi corner detection algorithm is used which returns some number of points satisfying this condition. Furthermore, for face tracking to work properly, the points selected must span the entire face of the user. This is relatively easily enforced by mandating a minimum distance between each of the points selected and by selecting enough points. 
\\ \\
It is conservative enough that the true position of the face is not lost, whilst still providing a 3x performance boost on the naive face detection approach. This is fully evaluated in section \ref{section:face_tracking}.

% \subsubsection{Camshift}
% \subsection{Face alignment}
%As opposed to 

\section{Region selection}
% DEFINE THE BASELINE: PRIMITIVEROI
% DISCUSS THE FUNDAMENTAL TRADEOFF BETWEEN NUMBER OF PIXELS AND FIDELITY
\label{ref:region_selection_impl}
Face detection systems, typically, return a bounding box, within which it is believed
a face is present. However, naturally, the box will also contain pixels from the background of 
the image since faces are not, in general, perfectly rectangular.  \\ \\
These background pixels will not contain any information as to the underlying heart rate of the user.
As a result, considering the entire bounding box will add unecessary noise to the resulting signal which consists of the mean colour from each region considered in the sequence of frames.
One approach might be to only consider skin pixels, however, robust, pose-invariant skin detection is a somewhat unsolved problem.
Furthermore, it is unclear whether all skin pixels are equally useful. For example, it might be that cheeks contain greater predictive power than the nose. Several approaches are presented regarding this problem with associated algorithms that are evaluated in section \ref{section:region_selection}.
% \subsection{Facial landmarks}
\subsection{Skin detection}
Suppose that we wish only to consider skin pixels. An obvious approach might be to apply an edge detection algorithm to isolate the boundary between the face and the background. However, edge detection algorithms, like the Canny edge detector, tend to produce large numbers of irrelevant edges and so were not implemented.
% \subsubsection{Clustering approaches}

%\subsection{}
%\subsection{Flood filling}
\subsubsection{Colour-based filtering}
\label{section:colour-filter}
Considering that skin tones tend to fall within a certain range of colours, one might encode this information in a primitive skin detector. For example, it is known that green is not a valid skin tone but brown might be. If a large enough number of skin tones are investigated then a range within which a skin pixel might lie could be defined. 
\\ \\
A dataset of $\sim$250000 skin and non-skin pixels was collected by Bhatt et al. \cite{skinDataset} and was sampled across a variety of skin tones, genders and ages. One could define the range of skin tones present in this dataset as a rudimentary skin detector. Clearly this will fail in many scenarios but serves as a useful baseline for comparison with more advanced techniques.
\\ \\

\begin{figure}[H]
    % \includegraphics[width=\textwidth]{example-image-b}
    \centering
    \input{implementation/skin-dataset-2.tex}
   \caption{A randomly sampled subset of the skin dataset represented in the Cb-Cr axes of YCbCr space} 
\end{figure}
\begin{figure}[H]
    % \includegraphics[width=\textwidth]{example-image-b}
    \centering
    \input{implementation/skin-dataset-1.tex}
   \caption{The range of skin tones present in the dataset with the skin and non-skin pixels plotted} 
\end{figure}
\paragraph{Issues}
This approach takes no consideration of the particular face that it is considering. Since it is not contexualised, it can fail in several circumstances.
For example, there are some hair colours which could be a skin tone on another individual but are clearly not on the particular example in question.
\begin{figure}[H]
    \includegraphics[width=\textwidth]{example-image-c}
   \caption{An example of the hair failure case} 
\end{figure}
Instead, an algorithm should attempt to identify skin on the particular face in question.
% EXPLAIN PERCEPTUAL UNIFORMITY 
% SHOW YCbCr REPRESENTATION OF SKIN PIXELS
% Talk about hair colour issue, doesn't work for lots of people, 
% Hence why we transition to figuring out skin tone
% crucially doesn't encode anything about the individual
%\subsection{Conditional Random Fields}

\subsubsection{K-Means}
If we consider how a human might identify skin, it could involve initially identifying the skin tone of the person and then assuming all parts of the face of a similar colour are in fact skin. This reduces the problem to identifying the skin tone in a face and then measuring the colour difference between each pixel and the skin tone. If the colours are within some specified threshold, then we can consider them to be skin pixels.
\\ \\
We might suppose that the image consists of clusters of pixels, some of which belong to the skin and others which don't. The center of the cluster of skin pixels represents the skin tone of the individual. Implicitly, this approach assumes that the Euclidean distance between points in our colour space is representative of the perceived colour difference. This property is known as perceptual uniformity and is not a property of all colour spaces. 
\\ \\
The colour space YCbCr is an approximation of perceptual uniformity and hence the image is converted from RGB before the application of clustering. 
% As a result, the colour space YCbCr happens to have this property and thus before any clustering is applied, the face image must be converted between these spaces. 
Under the assumption that our image consists of two clusters of pixels, skin and non-skin, we could simply apply 
the k-means algorithm to identify these clusters of pixels. Under the further assumption that the majority of pixels 
are skin pixels, we return the largest cluster as our set of skin pixels.
\\ \\

\begin{figure}[H]
    % Have an image from face to -> pixels in colour space on a graph with classification ->  to what classification looks like
    \includegraphics[width=\textwidth]{example-image-a}
   \caption{An example of the application of the k-means algorithm to skin detection} 
\end{figure}
%\subsubsection{Hierarchical clustering}
%\subsubsection{Markov clustering}
% \section{Ensemble Methods}


\paragraph{Issues}
% Plethora of failure cases

%The four main issues: doesn't consider location of pixels, selecting k not trivial and can impact (GIVE AN EXAMPLE), complexity, illumination resistance

% Takes no consideration of relative location of pixels and shape information so could degenerate in scenarios 
% where for example, there's a wallpaper with similar skin colour to skin tone
% Should be able to deal with this by understanding that skin pixels will occur in large patches
% A small number of pixels which aren't in the cluster surrounded by pixels that are shouldn't be wiped out
% There's some notion of skin pixels amongst others should be considered skin pixels

% Selection of the number of clusters not obvious
% Just using 2 works well for backgrounds where it's a single colour or skin pixels are very close together
% Give an example where there's a really specific colour present in the background in a large quantity

% Breaks with shadows
% Illumination resistance is extremely difficult how is it possible to tell the difference between specular reflection and white eyeballs?
% Assumption of most pixels being skin fails
The k-means algorithm, although it improves on the results of the rudimentary approach significantly, has a plethora of pitfalls.
\begin{itemize}
    \item Location: since it encodes no notion of location with respect to other pixels, a lone pixel in the corner that has a similar colour to the skin tone is considered the same as a pixel surrounded by skin pixels
    \item Number of clusters: there's no rigorous means for deciding the number of clusters to expect in the data and the selection of this value is critical 
    \item Performance: recall from Section \ref{section:system_design} that, since the region selection operates on every frame, it must run in real-time. However, in benchmarking the k-means reference implementation in the SKLearn library takes an order of magnitude longer than the minimum requirement for this constraint. \footnote{Suppose the camera streams at 30 frames per second, then each frame must be processed within 33ms, the reference implementation operates in the order of 100ms per frame. }
    \item Illumination resistance: the algorithm is not resistant to differences in illumination. For example, since there is no notion of location encoded, specular reflection on the forehead may not be considered as skin.
\end{itemize}

\begin{figure}[H]
    % Have an image from face to -> pixels in colour space on a graph with classification ->  to what classification looks like
    \includegraphics[width=\textwidth]{example-image-a}
   \caption{An example of the effect of differing the number of clusters} 
\end{figure}

% \subparagraph{Complexity}
% Not practical to be run for real-time applications, as is one of the requirements outlined in the first section

\subsection{Improving the previous approaches}
Recall that the purpose of the region selection is to return the mean colour of the pixels considered. The mean value from each frame is taken as a time series which is used to infer the heart rate. So rather than attempting to classify each pixel as either skin or not, I use a combination of the previous techniques to take a weighted mean which emphasises pixels which are believed to be skin pixels, in such a way as to minimise the issues encountered with the previous techniques.
\\ \\
Instead of applying k-means to each frame, which is not feasible with the constraint of real-time performance, we apply it periodically. One of the resulting clusters will correspond to the skin tone of the face in question.
Having identified the skin tone, this colour can be used as part of a probability model which we define to try and encode the likelihood of a particular pixel being part of the skin. This reduces the problem to: 
\begin{itemize}
   \item Use k-means to identify the skin tone
   \item Use the skin tone to identify skin in the face for some number of consecutive frames 
\end{itemize}
\subsubsection{Identifying the skin tone}
There are two expected properties of the cluster corresponding to the skin. It is likely to contain the largest number of pixels, since we'd expect most of the face to be composed of skin. It is also likely to fall within the range of expected human skin tones.
The difficulty of this problem, however, is that neither of these properties are guaranteed to hold.
For example, the face may be dominated by hair which could become the most prominent colour in the image. Likewise, changes in illumination or the presence of specular reflection might cause the skin tone of the face to be outside the expected range.
\\ \\
By considering both of these properties together, as a heuristic, it becomes more probable that the skin tone is identified correctly. I assign a score to each cluster, $C$, based on the number of elements in the cluster, $|C|$, and its distance from the expected range of skin tones, $d_C$.
\begin{equation*}
    \mathrm{score}(C) = \alpha\cdot |C| - \beta\cdot d_C
\end{equation*}
The constants $\alpha$ and $\beta$ were selected experimentally. The skin tone is identified as the center of the cluster with the maximum score.
This is shown to improve on simply selecting the largest cluster in Section \ref{section:skin_tone_detection}.

\subsubsection{Identifying skin pixels using the skin tone}
Identifying the skin tone, as described, requires the use of the k-means algorithm. However, as previously mentioned, this is not a particularly suitable algorithm for real-time applications. Thus, the skin tone can only be identified periodically rather than for every frame. So given the skin tone identified in some earlier frame, the value must be used to robustly detect skin pixels in subsequent frames. This is challenging since there will almost certainly be changing illumination conditions as well as changing positions of the skin pixels.
\\ \\
A possible approach could be to record the number of pixels that were in the skin cluster when the skin tone was identified. Then, our algorithm could select the $n$ closest pixels in the colour space to the skin tone, where $n$ was the number of pixels in the skin cluster. However, this would not be robust to rotations of the face, since, in subsequent frames, the number of pixels visible to the camera could change. Assuming a static value of $n$ would not be robust.
\\ \\
Implicitly, the above solution also relies on the assumption that any change in illumination between frames affects all pixels equally. 
It assumes that in subsequent frames, the true skin pixels remain closer (in terms of Euclidean distance) to the skin tone than the non-skin pixels.
%That is, the pixels which were within the skin cluster when k-means was applied, are still closer to the skin tone than non-skin pixels in subsequent frames. 
This is a fairly strong assumption and is affected by phenomena such as shadows and specular reflection, which will, clearly, not affect all pixels equally.
However, it is useful enough to make the problem more tractable than it was previously, without undermining the fidelity of the algorithm, if the skin tone is detected frequently enough. 
The validity of this assumption is made clear by the evaluation of the algorithm presented in Section \ref{section:skin_tone_detection}.
Therefore, we proceed by classifying pixels based on their Euclidean distance from the skin tone.
\\ \\
\paragraph{A Bayesian approach}
Recall that the first approach described classifies skin based on knowledge about the range of possible human skin tones. 
In this sense, it acts as a prior distribution, as, when classifying an individual pixel, it considers nothing of the face being presented.
The k-means implementation, on the other hand, considers the face presented but it has to knowledge of the prior. It instead assumes that the largest cluster must be the set of skin pixels.
\\ \\ 
A natural way to combine these two approaches would be to consider the problem of skin classification from a Bayesian perspective. Given some pixel $x_i$ we want to discover the likelihood of it being a skin pixel, having been conditionalised on the skin tone of the person as well the colour of the pixel being classified. 
We have access to our prior distribution from the first approach, which indicates the likelihood of a particular colour being skin.
Let us denote the following: 
\begin{itemize}
   \item $C_{\mathrm{skin}}$ as the classification of a pixel as skin
   \item $x_i$ the colour of the pixel being considered
   \item $s$ the skin tone of the face
\end{itemize} 
Given this notation the probability we wish to discover is precisely the following:
\begin{equation*}
   \Pr(C_{\mathrm{skin}}| x_i, s) 
\end{equation*}
Which, from Bayes' theorem, can be re-written as: 
\begin{align*}
   \frac{\Pr(C_\mathrm{skin}, x_i, s)}{\Pr(x_i, s)} = \frac{\Pr(s|C_\mathrm{skin}, x_i)\Pr(C_\mathrm{skin}|x_i)\Pr(x_i)}{\Pr(x_i,s)}
\end{align*}
By neglecting constants across both classes, classification can proceed using $\Pr(s|C_\mathrm{skin}, x_i)\Pr(C_\mathrm{skin}|x_i)$.
We proceed with classification as follows, for some threshold probability $t$ that defines the decision boundary: 
\begin{equation*}
    \text{isSkin}(x_i) = 
    \begin{cases}
        \texttt{True}, \quad \text{if} \Pr(s|C_\mathrm{skin}, x_i)\Pr(C_\mathrm{skin}|x_i) > t \\
        \texttt{False}, \quad \text{otherwise}
    \end{cases}
\end{equation*}
\paragraph{Defining the threshold}
The most obvious selection would be to classify the pixel based on which class has the maximum posterior probability. That is the threshold value $t$ would be $\Pr(C_\text{not-skin}| x_i, s)$, where $C_\text{not-skin}$ is the class of pixels classified as not skin. Since this is a binary classification task, this is equivalent to $t=1/2$.
% \begin{align*}
%     \Pr(C_\mathrm{skin}, x_i, s) &> \Pr(C_\mathrm{not-skin}, x_i, s) \\
%     \frac{\Pr(s|C_\mathrm{skin}, x_i)\Pr(C_\mathrm{skin}|x_i)\Pr(x_i)}{\Pr(x_i,s)} &> \frac{\Pr(s|C_\mathrm{not-skin}, x_i)\Pr(C_\mathrm{not-skin}|x_i)\Pr(x_i)}{\Pr(x_i,s)} \\
%     \Pr(s|C_\mathrm{skin}, x_i)\Pr(C_\mathrm{skin}|x_i) &> \Pr(s|C_\mathrm{not-skin}, x_i)\Pr(C_\mathrm{not-skin}|x_i) 
% \end{align*} 
% Since we're assuming that there are only two possible classes, this is equivalent to: 
% \begin{align*}
%     \Pr(s|C_\mathrm{skin}, x_i)\Pr(C_\mathrm{skin}|x_i) &> [1-\Pr(s|C_\mathrm{skin}, x_i)][1-\Pr(C_\mathrm{skin}|x_i)] \\
% \end{align*}
% So an equivalent condition in only a single class is: 
% \begin{equation*}
% \Pr(s|C_\mathrm{skin}, x_i)+\Pr(C_\mathrm{skin}|x_i) > 1
% \end{equation*}
% \Pr(s|C_\mathrm{skin}, x_i)\Pr(C_\mathrm{skin}|x_i) > t
\paragraph{Defining probability distributions}
Usually, one might attempt to learn these distributions from some relevant dataset. That is the prior distribution $\Pr(C_\mathrm{skin}|x_i)$ and the distribution $\Pr(s|C_\mathrm{skin}, x_i)$.
The prior $\Pr(C_\mathrm{skin}|x_i)$ was computed as the empirical distribution from 
the dataset discussed in Section \ref{section:colour-filter}. This dataset consists of randomly sampled pixels that are classified as skin or not, it does not include entire classified faces.
Furthermore, to the best of my knowledge at the time of implementation, there are no such adequately sized datasets. Hence, attempting to learn the distribution $\Pr(s|C_\mathrm{skin}, x_i)$ was not deemed to be feasible.
\\\\
As a result, a reasonable approach is to define the distribution $\Pr(s|C_\mathrm{skin}, x_i)$ based on the assumption that a skin pixel is likely close in colour to the overall skin tone. We want a function which returns a large probability if the Euclidean distance between the pixel and the skin tone is small.
There are, however, an infinite number of functions which could encode this. The only requirements is that the probability of being a skin pixel is decreasing as a function of the distance between the colour of the pixel and the skin tone of the face. 
\\\\
A reasonable assumption might be that if the pixel being considered, $x_i$, is a skin pixel, then the Euclidean distance between the skin tone, $s$ and the skin pixel, $d(s,x_i)$, varies as a Normal distribution with mean zero.
\begin{equation*}
   d(s, x_i) \sim N(0, \sigma) 
\end{equation*}
However, since colours are only represented in a finite interval, for example, between zero and one (or 0 and 255), this is not strictly correct. 
The above model has a non-zero probability associated with impossible skin-tones, since there are minimum and maximum possible distances.
As a result, I use a \textit{truncated} Normal distribution\footnote{A truncated distribution modifies the domain of a distribution by defining an updated probability density function which is zero outside a particular range $[a,b]$. This is achieved in such a way that the properties of a valid probability distribution are maintained.} instead.
\\\\
The probability density function of a truncated normal distribution between the values $a$ and $b$, mean $\mu$ and variance $\sigma$ is defined as: 
\begin{equation*}
    f(x) = \frac{1}{\sigma}\frac{\phi(\frac{x-\mu}{\sigma})}{\Phi(\frac{b-\mu}{\sigma}) - \Phi(\frac{a-\mu}{\sigma})}
\end{equation*}
\noindent
In this case, the values of $a$ and $b$ represent the minimum and maximum distances respectively from the given skin tone $s$. 
Hence, $a=0$, since the pixel being considered may have the same colour as the skin tone, in which case, $d(x_i, s) = d(s, s) = 0$.
The value of $b$ here is the distance between the skin tone and the furthest possible point in the colour space.
In this approach, a three dimensional colour space is used and, hence, the value of $b$ is the distance between $s$ and the furthest corner of the colourspace cube.
\\\\
Given this, the probability is defined in proportion to the probability density function of the truncated distribution for each possible skin tone.
\begin{equation*}
    \Pr(s|C_\mathrm{skin}, x_i) = \frac{f(d(s,x_i))}{\sum_{s' \in C}f(d(s',x_i))}
\end{equation*}
\\\\
\begin{figure}[H]
    \centering
    % Give an example image with prior etc.
    \includegraphics[width=\textwidth]{example-image-b}
   \caption{An example result from the describe skin detection algorithm} 
\end{figure}
%Discrete or not??
% There is a finite range of possible values for the Euclidean distance between the colour of a pixel and the skin tone. This is because colours are discretised by the camera and therefore must lie between some set of bounds (usually 0 and 255). As a result, the defined distribution should also be discrete in nature.
% \\\\ 

 
% \begin{equation}
%    \Pr(C_{\mathrm{skin}}| x_i) = \Pr(C_{\mathrm{skin}}| x_i, \text{skin-tone} )\Pr(\text{skin-tone})
% \end{equation}


%I deem it to be a strong enough assumption to simply the problem adequately but not so strong as to undermine the effectiveness of the solution. 


% Describe how this addresses each of the problems

% Run multiple times with different k, and look for colours that repeat in each one to identify skin tone
% Use rudimentary filter to remove obviously wrong ones
% Consider cluster size

% TALK ABOUT COMPLEXITY OF K-MEANS AND WHY USING IT REPEATEDLY DOESN'T WORK THAT WELL
% Detect skin tone every x frames 
% Then define a probability distribution based on d

% Morphologies
% The rudimentary colour-filtering approach provides a good baseline as to the 










\section{Heart rate isolation}
% EXPLAIN WHY IT'S NOT JUST THE FOURIER POWER SPECTRUM
% IN VIDEOS WITH MOVEMENT WE EXPECT HEART RATE TO BE A SERIES OF PEAKS TOGETHER RATHER THAN A SINGLE PEAK
% THAT MIGHT BE DUE TO LIGHTING ETC
% IMPACT OF LIGHTING CONDITIONS
Given a time series of mean colours in the region of the frame considered, inferring the heart rate might, naively, be taken as the prevalent frequency. That is, the largest peak in the Fourier transform. However, it is important to consider that the colours observed are not only the result of the underlying biological phenomenon of interest. Thereby this naive assumption is prone to returning, instead, the frequency of some other factor that impacts colour of the face. For example, respiration or movement of the face will have an impact, as well as any repetitive changes in lighting such as flickering. Isolating, the heart rate signal from the observed colour of the face, is a key part of the project.
\\\\
To simplify this approach the problem is broken down into two subtasks:
\begin{itemize}
    \item Identifying the pulse signal: given the noisy time series of observed colours for each frame, identify the signal corresponding to the pulse of the user
    \item Identifying the heart rate: given the pulse signal, identify the heart rate
\end{itemize}
Although, as will be explained, there is some overlap between the two to aide with correctly identifying the pulse.

\subsection{Blind-source separation}
Suppose that our observed colour signal, $\mathbf{I}(t)$, a vector-valued function, consists of a mixture of several underlying signals, $x_i(t)$ one of which is the pulse, $p(t)$. 
Our observed signal exists in three dimensions (i.e. the particular colour space in use), is the result of the mixing of three underlying signals by some mixing matrix $\mathbf{A}$.
\begin{equation*}
    \mathbf{I}(t) = \mathbf{A}\begin{bmatrix} x_1(t) \\ x_2(t) \\ x_3(t) \end{bmatrix}
\end{equation*}
% If we consider $\mathbf{I}(t)$ as a matrix of dimension $n \times T$ where $n$ is the number of colour dimensions and $T$ the number of timesteps, then we might suppose that $ (x_1(t), x_2(t), ..., p(t), ..., x_{n-1}(t))^T = AI$. That is 
% That is: $\mathbf{I}(t) = (x_1(t), x_2(t), x_3(t))$, for constants $\lambda_i$. 
% Alternatively we can consider this as finding the set of basis vectors $v_1, v_2, v_3$ such that $I(t) = (I(t)\cdot v_1)v_1 + (I(t) \cdot v_2)v_2 + (I(t) \cdot v_3)v_3$. The key problem is finding the basis $v_1, v_2, v_3$ such that $p(t) = (I(t)\cdot v_i))v_i$ for some $i$ and further identifying the value of $i$.
The nature of this task is to identify and return $p(t)$ from only $\mathbf{I}(t)$ and is known as the blind-source separation problem. 
\\ \\
The above formulation is not enough to isolate the signal $p(t)$. Thus assumptions must be placed on the nature of each $x_i(t)$ and $p(t)$ in order to turn this into a tractable problem. 
% \\ \\ 
% We can reframe this as finding some basis for our vector space $I(t) = \sum_{i}^{n}I(t)\cdot(v_i)v_i$

\subsubsection{Independent components analysis (ICA)}
A possible assumption might be that each of the $x_i(t)$ are statistically independent. That is, informally, one cannot gain any information about one of these signals given an other. This assumption encodes the notion that the pulse, should be entirely independent from the other phenomena impacting the observed signal.
\\\\
For example, suppose that the some of the $x_i(t)$ are the result of some physical phenomenon such as lighting conditions. In this case, it is intuitive to expect there to be no mutual information between the pulse and the other constituent signals. The ICA algorithm attempts to identify these signals based on this assumption, by using non-Gaussianity as a proxy for statistical independence.
\\\\
% talk about how this doesn't tell us which one is p(t) 
Crucially, however, this approach returns the signals $x_1(t)$, $x_2(t)$ and $x_3(t)$ but gives no indication regarding which signal corresponds to the pulse. In fact, the formulation of the ICA algorithm is such that each $x_i(t)$ are returned in a random order. Thus some additional work must be undertaken to identify the pulse from the returned signal. 
\\\\
In this scenario, I proceed by attempting to identify the heart rate in each of these signals independently. This results in returning three different heart rate values, each of which has an associated power.
This power value, which is the magnitude of the term in the Fourier transform associated with a given frequency, acts as a natural way of quantifying the importance of a particular frequency on the overall signal.
Hence, I assume that the correct heart rate corresponds to whichever has the largest power. This particular assumption is evaluated in Section \ref{section:ica_assumption}.
% For example, the pulse is likely to be the most periodic of these signals, in which case, we could identify the pulse as simply the 

\subsubsection{Principal components analysis (PCA)}
Alternatively, one might reframe the problem as finding a new orthogonal basis $\mathbf{v}_1, \mathbf{v}_2, \mathbf{v}_3$ to represent each vector $\mathbf{I}(t)$ in. The basis vectors are in the directions of maximal variance of the observed signal $\mathbf{I}(t)$. Finding these basis vectors is known as Principal Components Analysis.
\\\\
In this scenario, we assume that the pulse occurs in the direction of most variance.
Under the assumption that $\mathbf{v}_1, \mathbf{v}_2, \mathbf{v}_3$ are in order from most to least variance, we return $p(t) = \mathbf{I}(t)\cdot\mathbf{v}_1$.
Since we are only interested in the first basis vector, this is equivalent to fitting a plane to the three dimensional signal $\mathbf{I}(t)$ that minimises the squared error. The resulting pulse signal is the projection of $\mathbf{I}(t)$ onto this plane.
% ASSUMPTION OF CAMERA FEED BEING A MIXTURE OF INDEPENDENT SOURCES INCLUDING HEART RATE

% SELECTION OF RESULTING COMPONENT => HIGHEST PEAK

%SHOW IDEALISED HR AND WHY AUTOCORRELATION HELPS US PICK IT OUT


% \subsubsection{Principal components analysis}
% One might assume that each of the signals is 
% \subsection{Blind source separation}
%\subsection{Respiration rejection}

%\subsection{The sample-rate problem}
%\subsection{}


%talk about filtering stuff
\paragraph{Conclusion}
The evaluation of these two algorithms reflects whether the assumption of statistical independence or maximal variance better reflects the issue of isolating the heart rate. The performance of these two approaches are evaluated in Section \ref{section:bss}, from which it is concluded that ICA better serves the requirements of heart rate isolation. Hence, it is concluded that the assumption of statistical independence is more valid.
\subsection{Identifying the heart rate}
% given the pulse signal return HR
Given a pulse signal, the heart rate corresponds to the average number of beats of the heart (seen as spikes in the signal) in a minute. A possible approach to identifying this is to simply count the number of peaks in the signal. The heart rate could also be taken as the most prevalent frequency in the Fourier transform, that is, specifically, the frequency with the largest associated power. Since the Fourier transform provides a particularly rich representation from which to identify the heart rate, the latter approach is used. 
For example, the Fourier transform enables easy analysis of the relative powers of different peaks present in the power spectrum, which would not be easily possible with the former approach.

\subsubsection{The split peak issue}
%Talk about how heart rate can split over several peaks
%Main way to bypass this is to reward several close peaks together
%Hence the use of a butterworth filter
%Give EXAMPLE graphs
In practice, however, the true heart rate is not present in the resulting Fourier transform as a clear single peak. Instead, often, it will correspond to several peaks that are close by with smaller powers as is reported by van der Kooij et al. \cite{vanderKooij2019}. In order to avoid this, a Butterworth filter is applied which removes isolated peaks and increases the power of peaks close together as is recommended by van der Kooij et al. \cite{vanderKooij2019}.
\begin{figure}[H]
    % Add an image showing how in noisy Fourier transforms the HR is not just highest peak with result from Butterworth filter
    \includegraphics[width=\textwidth]{example-image-a}
   \caption{An example of the split peak issue with a Butterworth filter applied} 
\end{figure}

\section{Repository overview}