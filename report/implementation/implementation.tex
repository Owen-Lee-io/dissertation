\section{Overview}
% In this chapter I will outline the implementation 
% GIVE STRUCTURE OF PROGRAM AND EXPLAIN WHY MOST WORK IS DEDICATED TO REGION
% SELECTION AND HEART RATE ISOLATION SPECIFICALLY
% THAT IS FACE DETECTION IS MOSTLY A SOLVED PROBLEM
% BACKGROUND ON HOW DESIGN IS TO MIMIC A PPG SENSOR
Abstractly, the program consists of three distinct tasks, each of which rely on the result from the previous. Together, forming a kind of pipeline:
\begin{itemize}
    \item \textbf{Face detection}: identify a face in each supplied camera frame
    \item \textbf{Region selection}: given a bounding box around a face, select some set of pixels to consider 
    \item \textbf{Heart rate isolation}: given a time series of the mean colour of some region of the face, infer the heart rate
\end{itemize}
Face detection is largely a solved problem and thereby the project mostly concentrates on the latter two, for which, there is very much ongoing research.

\section{System design}
Each of these tasks occur at different rates and, thereby, have different performance constraints which must be upheld. Face detection and region selection operate on every frame received from the camera and so must run in real-time, or risk dropping streamed frames.
\\ \\
Heart rate isolation, however, is only executed after some adequate number of data points is received and is recomputed after a fixed time. Since none of the prior stages rely on the estimated heart rate, its execution time requirements are less stringent. This is exploited to perform relatively expensive analyses without slowing earlier stages. Crucially, this relies on the assumption that it can be run concurrently from the earlier stages.
% The system was designed with a focus on decomposition. 
Separating these tasks allows for this concurrency to be implemented safely.

\begin{figure}[!h]
    \includegraphics[width=\textwidth]{example-image-a}
   \caption{A diagram representing the design of the system} 
\end{figure}
\noindent
The camera streams frames to the \textbf{FaceDetector}. The \textbf{RegionSelector} then takes the mean pixel colour of the region considered and adds this value to the dataset. Once the appropriate number of values is reached, the \textbf{HRIsolator} spawns a new thread, the \textbf{IsolationTask}, which attempts to infer the heart rate from the values collected. 
\\ \\
Although the region selection could also be executed in a separate thread, the associated setup costs are likely to have an adverse impact on real-time performance. However, one might wish to use more expensive region selection algorithms which cannot run in real-time. In these scenarios, the program could copy the frame and execute the selection in a separate thread from the main face detection loop. This is fundamentally a tradeoff between memory usage and execution time. Furthermore, since there are only a finite number of threads which can be spawned, quickly the limit might be reached without providing much additional computation time to the \textbf{RegionSelector}. As a result, the \textbf{FaceDetector} and \textbf{RegionSelector} operate sequentially in the same thread.
% \subsection{Asynchronous execution}
% TALK ABOUT CONCURRENCY RUNNING HR ISOLATION ASYNCHRONOUSLY
\section{Face detection}
% TALK ABOUT CRITICAL NOT TO DROP FRAMES
A face detector is expected to take a single frame and return a bounding rectangle within which a face is present. This could be extended to work on a stream of frames, naÃ¯vely, by simply repeated applying this face detector to each frame independently.

\subsection{Face tracking}
Smartphone cameras can readily stream at high framerates, so it is unlikely that a face moves very much between a pair of consecutive frames. At sixty frames per second, frames are recorded only 0.017s apart. The position of the face in the previous frame, gives a strong indication of its subsequent position. Thus, there is opportunity for optimisation beyond simply applying a face detector to each frame individually. This is important since a speedup in this part of the pipeline provides opportunity for more expensive computation in subsequent stages which might improve accuracy. However, it is critical that any optimisations are resistant to motion. Using information from previous frames forms the distinction between face \textbf{\textit{detection}} and \textbf{\textit{tracking}}.

\subsubsection{Point tracking}
The key principle behind face tracking is to use information from previous frames to reduce the cost of subsequent face detections.
To that end, I implemented an optical-flow based algorithm that tracks points on the face rather than repeatedly calling the face detector. The Lucas-Kanade algorithm, a particular variant of optical-flow, tracks a set of points between consecutive frames. Naturally, over time, these points will diverge from their true positions. This is because images contain large numbers of pixels with similar illumination, over time, these cannot be distinguished perfectly. When this occurs, the position of the face should be redetected. 
\\ \\
Knowing when the points have diverged is non-trivial, since the true location of the face is unknown. It is crucial for any implementation to act safely enough that the face is not lost track of, whilst simultaneously minimising the number of redetections.
There are two obvious approaches to combat this. The algorithm could redetect the face: 
\begin{itemize}
    \item periodically 
    \item when the tracked face changes in size significantly. 
\end{itemize}
The former wastes computation time for stationary videos where the redetections might be unnecessary. Simultaneously, the time between redetections must be short enough to deal with videos with significant movement. This static approach, therefore, was not considered. For this reason, the latter was implemented. We track the face and if the size of the box surrounding it changes significantly, indicating that our confidence in the tracked points has reduced, then we recompute the true position of the face.
\begin{figure}[!h]
\begin{minted}{python}
   points = []
   last_detection = None
   def face_tracker(frame):
        if redetect or points is empty:
            face = face_detector(frame) 
            last_detection = face
            points = select_new_points(face)
        else:
            points = track_points(points)
            face = bounding_box(points)
        redetect = change_in_size(last_detection, face) > threshold
        previous_face = face
        return face
\end{minted}
\caption{A simplified pseudocode representation of an optical-flow based face tracker}
\end{figure} 
\paragraph{Impact of the rate of redetections}
It is important to reason about precisely when face tracking provides a performance boost. To understand this let us consider a sequence of frames which the face tracker has been applied to. The cost of tracking is compared with that of repeatedly detecting the face in each frame independently. Under the following notation:
\begin{itemize}
   \item $W$: number of consecutive frames considered 
   \item $R$: the total number of redetections by the face tracker
   \item $n$: size of each frame in pixels
   \item $p$: number of points tracked
   \item $f(n)$: cost of a face detection on a single frame of size $n$
   \item $s(p,f)$: cost of selecting $p$ points to track in a face of size $f$
   \item $g(p,n)$: cost of tracking $p$ points in a frame of size $n$
\end{itemize}
The cost of the repeated face detector is $Wf(n)$. Since, for each frame in the sequence, it detects the face independently and incurs a cost of $f(n)$.  
\\ \\
The point tracking approach instead has a cost of $Wg(p,n) + R(f(n) + s(p,f))$. For each redetection it calls the face detector and selects a new set of points. It also tracks the set of points for every frame, hence the term $Wg(p,n)$. In the worst case, $R=W$, so there is no saving in terms of computational complexity. Instead let us investigate for what values of $R$ there is a cost saving.
\begin{align*}
    Wf(n) &> Wg(p,n) + R[f(n) + s(p,f)] \\
    R &< \frac{W[f(n) - g(p,n)]}{f(n) + s(p,f)}\\
    \frac{R}{W} &< \frac{f(n)-g(p,n)}{f(n)+s(p,f)}
\end{align*}
Notice that $R/W$ represents the percentage of frames for which a redetection occurs. There is only a performance saving when this percentage falls below the value on the right hand side. Furthermore, this value itself is based on the relative costs of face detection and of selecting and tracking points. Implicitly, the algorithm assumes that the cost of tracking and selecting points is less than that of face detection, otherwise, this endeavour would be useless. This assumption is validated and the limit is evaluated experimentally in section \ref{section:face_tracking}. From this it is shown that even for videos with lots of movement, the value $R/W$ falls below this limit and the inequality is satisfied. 
% The algorithm above works by selecting some number of points on the face and tracking them between frames. If the points move such that the size of the bounding box increases above some threshold from the last face detection, then the face detector is called again. 
% \paragraph{Assumptions}
% This relies on the assumption that if the points lose track of the face, then the threshold is such that 
% talk about probability of a translation which is the main failure case under the assumption of constant size between frames
% the alternative assumption would be to assume a stationary center but comment that in practice points tend to move outwards since some points will 
\paragraph{Details}
To maximise the performance of the optical flow algorithm, selecting points randomly will not suffice. That is because, points which are particularly different to their neighbours, for example, are much easier to track. To achieve this, the Shi-Tomasi corner detection algorithm is used which returns some number of points satisfying this condition. Furthermore, for face tracking to work properly, the points selected must span the entire face of the user. This is relatively easily enforced by mandating a minimum distance between each of the points selected and by selecting enough points. 
\\ \\
It is conservative enough that the true position of the face is not lost, whilst still providing a 3x performance boost on the naive face detection approach. This is fully evaluated in section \ref{section:face_tracking}.

\subsubsection{Camshift}
\subsection{Face alignment}
%As opposed to 

\section{Region selection}
% DEFINE THE BASELINE: PRIMITIVEROI
% DISCUSS THE FUNDAMENTAL TRADEOFF BETWEEN NUMBER OF PIXELS AND FIDELITY
Face detection systems, typically, return a bounding box, within which it is believed
a face is present. However, naturally, the box will also contain pixels from the background of 
the image since faces are not, in general, perfectly rectangular.  \\ \\
These background pixels will not contain any information as to the underlying heart rate of the user.
As a result, considering the entire bounding box will add unecessary noise to the resulting signal which consists of the mean colour from each region considered in the sequence of frames.
One approach might be to only consider skin pixels, however, robust, pose-invariant skin detection is a somewhat unsolved problem.
Furthermore, it is unclear whether all skin pixels are equally useful. For example, it might be that cheeks contain greater predictive power than the nose. Several approaches are presented regarding this problem with associated algorithms that are evaluated in section \ref{section:region_selection}.
% \subsection{Facial landmarks}
\subsection{Skin detection}
Suppose that we wish only to consider skin pixels. An obvious approach might be to apply an edge detection algorithm to isolate the boundary between the face and the background. However, edge detection algorithms, like the Canny edge detector, tend to produce large numbers of irrelevant edges.

% \subsubsection{Clustering approaches}

%\subsection{}
%\subsection{Flood filling}
\subsubsection{Colour-based filtering}
Considering that skin tones tend to fall within a certain range of colours, one might encode this information in a primitive detector. For example, it is known that green is not a valid skin tone but brown might be. If a large enough number of skin tones are investigated then one could define a range within which a skin pixel might lie \cite{skinDataset}.



\paragraph{Issues}

% EXPLAIN PERCEPTUAL UNIFORMITY 
% SHOW YCbCr REPRESENTATION OF SKIN PIXELS
% Talk about hair colour issue, doesn't work for lots of people, 
% Hence why we transition to figuring out skin tone
% crucially doesn't encode anything about the individual
%\subsection{Conditional Random Fields}

\subsubsection{K-Means}
% TALK ABOUT COMPLEXITY OF K-MEANS AND WHY USING IT REPEATEDLY DOESN'T WORK THAT WELL

%\subsubsection{Hierarchical clustering}
%\subsubsection{Markov clustering}
% \section{Ensemble Methods}

\section{Heart rate isolation}
% EXPLAIN WHY IT'S NOT JUST THE FOURIER POWER SPECTRUM
% IN VIDEOS WITH MOVEMENT WE EXPECT HEART RATE TO BE A SERIES OF PEAKS TOGETHER RATHER THAN A SINGLE PEAK
% THAT MIGHT BE DUE TO LIGHTING ETC
% IMPACT OF LIGHTING CONDITIONS
Given a time series of mean colours in the region of the frame considered, inferring the heart rate might, naively, be taken as the prevalent frequency. That is, the largest peak in the Fourier transform. However, it is important to consider that the colours observed are not only the result of the underlying biological phenomenon of interest. Thereby this naive assumption is prone to returning, instead, the frequency of some other factor that impacts colour of the face. For example, respiration or movement of the face will have an impact, as well as any repetitive changes in lighting such as flickering. Isolating, the heart rate signal from the observed colour of the face, is the crux of this project. 

\subsection{Blind-source separation}
Suppose that our observed colour signal, $\mathbf{I}(t)$, a vector-valued function, consists of a mixture of several underlying signals, $x_i(t)$ one of which is the pulse, $p(t)$. If we consider $\mathbf{I}(t)$ as a matrix of dimension $n \times T$ where $n$ is the number of colour dimensions and $T$ the number of timesteps, then we might suppose that $ (x_1(t), x_2(t), ..., p(t), ..., x_{n-1}(t))^T = AI$. That is 
That is: $\mathbf{I}(t) = (x_1(t), x_2(t), x_3(t))$, for constants $\lambda_i$. 
Alternatively we can consider this as finding the set of basis vectors $v_1, v_2, v_3$ such that $I(t) = (I(t)\cdot v_1)v_1 + (I(t) \cdot v_2)v_2 + (I(t) \cdot v_3)v_3$. The key problem is finding the basis $v_1, v_2, v_3$ such that $p(t) = (I(t)\cdot v_i))v_i$ for some $i$ and further identifying the value of $i$.

The nature of this task is to identify and return $\mathbf{p}(t)$ from only $\mathbf{I}(t)$. This is known as the blind-source separation problem. 
\\ \\
The above formulation is not enough to isolate the signal $\mathbf{p}(t)$. Thus assumptions must be placed on the nature of each $\mathbf{x}_i(t)$ and $\mathbf{p}(t)$ in order to turn this into a tractable problem. 
\\ \\ 
We can reframe this as finding some basis for our vector space $I(t) = \sum_{i}^{n}I(t)\cdot(v_i)v_i$

\subsubsection{Independent components analysis}
A possible assumption might be that each of the $\mathbf{x}_i(t)$ and $\mathbf{p}(t)$ are statistically independent. That is, informally, one cannot gain any information about one of these signals given another.
\subsubsection{Principal components analysis}
One might assume that each of the signals is 

% ASSUMPTION OF CAMERA FEED BEING A MIXTURE OF INDEPENDENT SOURCES INCLUDING HEART RATE

% SELECTION OF RESULTING COMPONENT => HIGHEST PEAK

%SHOW IDEALISED HR AND WHY AUTOCORRELATION HELPS US PICK IT OUT


\subsection{Respiration rejection}

\subsection{The sample-rate problem}
%\subsection{}

\subsection{The split peak issue}

\section{Repository overview}