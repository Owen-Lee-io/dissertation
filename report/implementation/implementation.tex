% \section{Overview}
% In this chapter I will outline the implementation 
% GIVE STRUCTURE OF PROGRAM AND EXPLAIN WHY MOST WORK IS DEDICATED TO REGION
% SELECTION AND HEART RATE ISOLATION SPECIFICALLY
% THAT IS FACE DETECTION IS MOSTLY A SOLVED PROBLEM
% BACKGROUND ON HOW DESIGN IS TO MIMIC A PPG SENSOR
Abstractly, the program consists of three distinct tasks, each of which rely on the result from the previous. Together, forming a kind of pipeline:
\begin{itemize}
    \item \textbf{Face detection}: identify a face in each supplied camera frame.
    \item \textbf{Region selection}: given a bounding box around a face, select some set of pixels to consider which are amenable to heart rate detection.
    \item \textbf{Heart rate isolation}: given a time series of the mean colour of each frame infer the heart rate.
\end{itemize}
Face detection is largely a solved problem and thereby the project mostly concentrates
on the implementation of the latter two tasks which are the topic of ongoing research.

% \section{System design}
% \label{section:system_design}
% These three tasks occur at different rates and, thereby, have different performance constraints which must be upheld. Face detection and region selection operate on every frame received from the camera and so must run in real-time, or risk dropping streamed frames.
% \\ \\
% Heart rate isolation, however, is only executed after an adequate number of data points are received (denoted as the `window size') and is recomputed after a fixed time. Since none of the prior stages rely on the estimated heart rate, its execution time requirements are less stringent. This is exploited to perform relatively expensive analyses without slowing earlier stages. Crucially, this relies on the assumption that it can be run concurrently.
% % The system was designed with a focus on decomposition. 
% Separating these tasks allows for this concurrency to be implemented safely.

% \begin{figure}[H]
%     % \includegraphics[width=\textwidth]{example-image-a}
%     \centering
%     \scalebox{0.7}{\input{implementation/uml.tex}}
%    \caption{A UML sequence diagram showcasing the use of threading} 
% \end{figure}
% \noindent
% The camera streams frames to the \textbf{FaceDetector}. The \textbf{RegionSelector} then takes the mean pixel colour of the region considered and adds this value to the dataset. Once enough values have been collected, as defined by the window size, the \textbf{HRIsolator} spawns a new thread, the \textbf{IsolationTask}, which attempts to infer the heart rate from the window of values. 
% \\ \\
% % The region selection could also be executed in a separate thread, however the associated setup costs are likely to have an adverse impact on real-time performance. 
% One might wish to use more expensive region selection algorithms which cannot run in real-time. In these scenarios, the program could copy the frame and execute the selection in a separate thread from the main face detection loop. However, it is not clear as to whether the cost of synchronisation between threads would nullify any speedup and so this approach was not pursued.

% In fact, synchronisation costs between threads may nullify any 
% Furthermore, since there are only a finite number of threads which can be spawned, quickly the limit might be reached without providing much additional computation time to the \textbf{RegionSelector}. As a result, the \textbf{FaceDetector} and \textbf{RegionSelector} operate sequentially in the same thread.
% \subsection{Asynchronous execution}
% TALK ABOUT CONCURRENCY RUNNING HR ISOLATION ASYNCHRONOUSLY
\section{Face detection}
% TALK ABOUT CRITICAL NOT TO DROP FRAMES
A face detector is expected to take a single frame and return a bounding rectangle within which a face is present. This could be extended to work on a stream of frames, naÃ¯vely, by simply repeated applying this face detector to each frame independently.

\subsection{Face tracking}
\label{section:face_tracking_impl}
Smartphone cameras can readily stream at high frame rates, so it is unlikely that a face moves very much between a pair of consecutive frames. At thirty frames per second, frames are recorded only 0.033s apart. The position of the face in the previous frame, gives a strong indication of its subsequent position. Thus, there is opportunity for optimisation beyond simply applying a face detector to each frame individually. 
A speedup in this part of the pipeline provides opportunity for more expensive computation in subsequent stages which might improve accuracy. However, it is critical that any optimisations are resistant to motion. Using information from previous frames forms the distinction between face \textbf{\textit{detection}} and \textbf{\textit{tracking}}.

\subsubsection{Point tracking}
The key principle behind face tracking is to use information from previous frames to reduce the cost of subsequent face detections.
To that end, I use the Lucas-Kanade algorithm \cite{LucasKanade}, as described in the Preparation chapter, to track points on the face rather than repeatedly calling the face detector. Naturally, over time, these points will diverge from their true positions. This is because images contain large numbers of pixels with similar illumination that cannot be distinguished perfectly. When this occurs, the position of the face should be redetected. 
\\ \\
Knowing when the points have diverged is non-trivial, since the true location of the face is unknown, without calling the face detector directly. It is crucial for any implementation to act safely enough that the face is not lost track of, whilst simultaneously minimising the number of times the face detection algorithm is used.
There are two obvious approaches to combat this. The algorithm could redetect the face: 
\begin{itemize}
    \item periodically 
    \item when the tracked face changes in size significantly. 
\end{itemize}
The former wastes computation time for stationary videos where the redetections might be unnecessary. Simultaneously, the time between redetections must be short enough to deal with videos with significant movement. This static approach, therefore, was not considered and the latter was implemented. The face is tracked and if the size of the box surrounding it changes significantly, the true position of the face is recomputed. The definition of a `significant' change is arrived at by experimentation (see Section \ref{section:face_tracking}).
%%TC:ignore 
\begin{figure}[H]
\centering
\begin{minted}{python}
   points = []
   redetect = True
   last_detection = None
   def face_tracker(frame):
        if redetect or points is empty:
            face = face_detector(frame) 
            last_detection = face
            points = select_new_points(face)
        else:
            points = track_points(points)
            face = bounding_box(points)
        redetect = change_in_size(last_detection, face) > threshold
        last_detection = face
        return face
\end{minted}
%%TC:endignore 
\caption{A simplified pseudocode representation of an optical-flow based face tracker}
\end{figure} 
\paragraph{Impact of the rate of redetections}
It is important to reason about precisely when face tracking provides a performance boost. To understand this let us consider a sequence of frames which the face tracker has been applied to. The cost of tracking is compared with that of repeatedly detecting the face in each frame independently. Under the following notation:
\begin{itemize}
   \item $W$: number of consecutive frames considered 
   \item $R$: the total number of redetections by the face tracker
   \item $n$: size of each frame in pixels
   \item $p$: number of points tracked
   \item $f(n)$: cost of a face detection on a single frame of size $n$
   \item $s(p,f)$: cost of selecting $p$ points to track in a face of size $f$
   \item $g(p,n)$: cost of tracking $p$ points in a frame of size $n$
\end{itemize}
The cost of the repeated face detector is $Wf(n)$. Since, for each frame in the sequence, it detects the face independently and incurs a cost of $f(n)$.  
\\ \\
The point tracking approach instead has a cost of $Wg(p,n) + R(f(n) + s(p,f))$. For each redetection it calls the face detector and selects a new set of points. It also tracks the set of points for every frame, hence the term $Wg(p,n)$. Let us investigate for what values of $R$ there is a cost saving.
\begin{equation}
    Wf(n) > Wg(p,n) + R[f(n) + s(p,f)] 
    \label{eq:saving}
\end{equation}
This can be rewritten in terms of the rate of redetection $R/W$ by rearrangement of equation \ref{eq:saving}.
\begin{equation}
    \frac{R}{W} < \frac{f(n) - g(p,n)}{f(n) + s(p,f)}
\end{equation}
 There is only a performance saving when this rate falls below the value on the right hand side which itself depends on the relative costs of face detection, selecting and tracking of points. Implicitly, the algorithm assumes that the cost of tracking and selecting points is less than that of face detection, otherwise, there would be no time saving. This assumption is validated and the limit is evaluated experimentally in Section \ref{section:face_tracking}. It is shown that even for videos with lots of movement, the value $R/W$ falls below this limit and the inequality is satisfied. 
% The algorithm above works by selecting some number of points on the face and tracking them between frames. If the points move such that the size of the bounding box increases above some threshold from the last face detection, then the face detector is called again. 
% \paragraph{Assumptions}
% This relies on the assumption that if the points lose track of the face, then the threshold is such that 
% talk about probability of a translation which is the main failure case under the assumption of constant size between frames
% the alternative assumption would be to assume a stationary center but comment that in practice points tend to move outwards since some points will 
% \paragraph{Details}
% For face tracking to work properly, the points selected must span the entire face of the user. This is relatively easily enforced by mandating a minimum distance between each of the points selected and by selecting enough points. 
% \\ \\
\\\\
In Section \ref{section:face_tracking}, I show that face tracking introduces no statistically significant effect on the accuracy of heart rate predictions whilst providing a 3x performance speedup regardless of the amount of movement in the video.
% \subsubsection{Camshift}
% \subsection{Face alignment}
%As opposed to 

\section{Region selection}
% DEFINE THE BASELINE: PRIMITIVEROI
% DISCUSS THE FUNDAMENTAL TRADEOFF BETWEEN NUMBER OF PIXELS AND FIDELITY
\label{ref:region_selection_impl}
The bounding box returned by face detection will contain pixels from the background of 
the image since faces are not, in general, perfectly rectangular. 
These background pixels do not contain any information as to the underlying heart rate of the user and so considering the entire bounding box adds unecessary noise to the resulting signal.
The precise set of pixels we wish to discover are those belonging to the skin of the user's face, however, robust, pose-invariant skin detection is non-trivial.
% Nonetheless, I proceed by attempting to maximise the number of relevant pixels considered by the implementation.

% Furthermore, it is unclear whether all skin pixels are equally useful. For example, it might be that cheeks contain greater predictive power than the nose. Several approaches are presented regarding this problem with associated algorithms that are evaluated in section \ref{section:region_selection}.
% \subsection{Facial landmarks}
\subsection{Skin detection}
 An obvious approach to skin detection might be to apply an edge detection algorithm to isolate the boundary between the face and the background. However, edge detection algorithms, like the Canny edge detector \cite{canny}, tend to produce large numbers of irrelevant edges and so were not considered.
% \subsubsection{Clustering approaches}

%\subsection{}
%\subsection{Flood filling}
\subsubsection{Colour-based filtering}
\label{section:colour-filter}
Considering that skin tones tend to fall within a certain range of colours, one might encode this information in a primitive skin detector. For example, it is known that green is not a valid skin tone but brown might be. If a large enough number of skin tones are investigated then a range within which a skin pixel might lie could be defined. 
\\ \\
A dataset of $\sim$250000 skin and non-skin pixels was collected by Bhatt et al.~\cite{skinDataset} and was sampled across a variety of skin tones, genders and ages. This is plotted in Figure \ref{fig:skin_ycbcr}, with the range of skin pixels plotted in Figure \ref{fig:skin_ycbcr_zoomed}. One could define the range of skin tones present in this dataset as a rudimentary skin detector. Clearly this will fail in many scenarios but serves as a useful baseline for comparison with more advanced techniques.

\begin{figure}
    % \includegraphics[width=\textwidth]{example-image-b}
    \centering
    \scalebox{0.8}{ \input{implementation/skin-dataset-2.tex}}
   \caption{\textit{A randomly sampled subset of the skin dataset represented in the Cb-Cr axes of YCbCr space} }
   \label{fig:skin_ycbcr}
\end{figure}
\begin{figure}
    % \includegraphics[width=\textwidth]{example-image-b}
    \centering
    \scalebox{0.8}{\input{implementation/skin-dataset-1.tex}}
   \caption{\textit{The range of skin tones present in the dataset with the skin and non-skin pixels plotted} }
   \label{fig:skin_ycbcr_zoomed}
\end{figure}
\paragraph{Issues}
This approach takes no consideration of the particular face in question. Since it is not contexualised, it can fail in several circumstances.
For example, there are some hair colours which could be a skin tone on another individual but are clearly not on the particular user in question (see Figure \ref{fig:hair_failure}).
Instead, an algorithm should attempt to identify skin on the particular face in question.
\begin{figure}[H]
    \centering
    \subfloat[Original image\protect\footnotemark  ]{{\includegraphics[width=5cm]{implementation/c.jpg} }}%
    \qquad
    \subfloat[Mask indicating skin pixels]{{\includegraphics[width=5cm]{implementation/interval.jpg} }}%
   \caption{\textit{An example of the hair failure case}}
   \label{fig:hair_failure}
\end{figure}
% \footnotemark[1]
\footnotetext[1]{ An image of mathematician Katherine Johnson taken from the public domain.}
% EXPLAIN PERCEPTUAL UNIFORMITY 
% SHOW YCbCr REPRESENTATION OF SKIN PIXELS
% Talk about hair colour issue, doesn't work for lots of people, 
% Hence why we transition to figuring out skin tone
% crucially doesn't encode anything about the individual
%\subsection{Conditional Random Fields}

\subsubsection{K-Means}
If we consider how a human might identify skin, it could involve initially identifying the skin tone of the person and then assuming all parts of the face of a similar colour are in fact skin. This reduces the problem to identifying the skin tone in a face and then measuring the colour difference between each pixel and the skin tone. If the colours are within some specified threshold, then we can consider them to be skin pixels.
\\ \\
We might suppose that the image consists of clusters of pixels, some of which belong to the skin and others which don't. The center of the cluster of skin pixels represents the skin tone of the individual. Implicitly, this approach assumes that the Euclidean distance between points in our colour space is representative of the perceived colour difference. This property is known as perceptual uniformity and is not a property of all colour spaces. 
\\ \\
The colour space YCbCr is an approximation of perceptual uniformity and hence the image is converted from RGB before the application of clustering. 
% As a result, the colour space YCbCr happens to have this property and thus before any clustering is applied, the face image must be converted between these spaces. 
Under the assumption that our image consists of two clusters of pixels, skin and non-skin, we could simply apply 
the k-means algorithm\footnote{A popular algorithm for discovering clusters in data} \cite{kmeans} to identify these clusters. Under the further assumption that the majority of pixels 
are skin pixels, the largest cluster is returned as the set of skin pixels (see Figure \ref{fig:kmeans}).

\begin{figure}[H]
    % Have an image from face to -> pixels in colour space on a graph with classification ->  to what classification looks like
    \centering
    \subfloat[Original image]{{\includegraphics[width=5cm]{implementation/c.jpg} }}%
    \qquad
    \subfloat[Mask indicating regions considered to be skin.]{{\includegraphics[width=5cm]{implementation/r2.jpg} }}%
    % \includegraphics[width=0.5\textwidth]{example-image-a}
   \caption{\textit{An example of the application of the k-means algorithm to skin detection}}
   \label{fig:kmeans}
\end{figure}
%\subsubsection{Hierarchical clustering}
%\subsubsection{Markov clustering}
% \section{Ensemble Methods}


\paragraph{Issues}
\label{impl:kmeans_bad}
% Plethora of failure cases

%The four main issues: doesn't consider location of pixels, selecting k not trivial and can impact (GIVE AN EXAMPLE), complexity, illumination resistance

% Takes no consideration of relative location of pixels and shape information so could degenerate in scenarios 
% where for example, there's a wallpaper with similar skin colour to skin tone
% Should be able to deal with this by understanding that skin pixels will occur in large patches
% A small number of pixels which aren't in the cluster surrounded by pixels that are shouldn't be wiped out
% There's some notion of skin pixels amongst others should be considered skin pixels

% Selection of the number of clusters not obvious
% Just using 2 works well for backgrounds where it's a single colour or skin pixels are very close together
% Give an example where there's a really specific colour present in the background in a large quantity

% Breaks with shadows
% Illumination resistance is extremely difficult how is it possible to tell the difference between specular reflection and white eyeballs?
% Assumption of most pixels being skin fails
The k-means algorithm, although it improves on the results of the rudimentary approach has a plethora of pitfalls.
\begin{itemize}
    \item \textbf{Performance}: the region selection algorithm operates on every frame of data and so must run in real-time. However, in benchmarking the k-means reference implementation in the scikit-learn \cite{sklearn} library takes an order of magnitude longer than the minimum requirement for this constraint\footnote{Suppose the camera streams at 30 frames per second, then each frame must be processed within 33ms, the reference implementation operates in the order of 100ms per frame. }.
    \item \textbf{Location}: since it encodes no notion of location with respect to other pixels, a lone pixel in the corner that has a similar colour to the skin tone is considered the same as a pixel surrounded by skin pixels.
    \item \textbf{Number of clusters}: there's no rigorous means for deciding the number of clusters to expect in the data and the selection of this value is critical (see Figure \ref{fig:kmeans_clusters}). Too many clusters can cause unexpected behaviour, but too few can result in undesired pixels being considered. 
    \item \textbf{Illumination resistance}: the algorithm is not resistant to differences in illumination. For example, since there is no notion of location encoded, specular reflection on the forehead may not be considered as skin.
\end{itemize}

\begin{figure}[H]
    \centering
    % Have an image from face to -> pixels in colour space on a graph with classification ->  to what classification looks like
    % \includegraphics[width=0.5\textwidth]{example-image-a}
    \subfloat[Original image]{{\includegraphics[width=3cm]{implementation/c.jpg} }}%
    \qquad
    \subfloat[Clustering w/ k=2]{{\includegraphics[width=3cm]{implementation/r2.jpg} }}%
    \qquad
    \subfloat[Clustering w/ k=3]{{\includegraphics[width=3cm]{implementation/r3.jpg} }}%
    \qquad
    \subfloat[Clustering w/ k=4]{{\includegraphics[width=3cm]{implementation/r4.jpg} }}%

    \subfloat[Original image\protect\footnotemark]{{\includegraphics[width=3cm]{implementation/c-2.jpg} }}%
    \qquad
    \subfloat[Clustering w/ k=2]{{\includegraphics[width=3cm]{implementation/r2-2.jpg} }}%
    \qquad
    \subfloat[Clustering w/ k=3]{{\includegraphics[width=3cm]{implementation/r3-2.jpg} }}%
    \qquad
    \subfloat[Clustering w/ k=4]{{\includegraphics[width=3cm]{implementation/r4-2.jpg} }}%
   \caption{\textit{An example of the effect of differing the number of clusters} }
   \label{fig:kmeans_clusters}
\end{figure}
\footnotetext[3]{An image of astronaut Neil Armstrong from the public domain. }
% \subparagraph{Complexity}
% Not practical to be run for real-time applications, as is one of the requirements outlined in the first section

\subsection{Improving the previous approaches}
Recall that the purpose of the region selection algorithm is to return the mean colour of the pixels considered. The mean colour from each frame is taken as a time series which is used to infer the heart rate. So rather than attempting to classify each pixel as either skin or not, I use a combination of the previous techniques to take a weighted mean which emphasises pixels which are believed to be skin pixels, in such a way as to minimise the issues encountered with the previous techniques.
\\ \\
Instead of applying k-means to each frame, which is not feasible with the constraint of real-time performance, we apply it once at the start of the video.
One of the resulting clusters will correspond to the skin tone of the face in question.
Having identified the skin tone, this colour can be used as part of a probability model which is defined to encode the likelihood of a particular pixel being part of the skin. 
In this way, the cost of skin detection is amortized over the entire video rather than per frame.
This reduces the problem to: 
\begin{itemize}
   \item Apply k-means to identify the skin tone.
   \item Use the skin tone to identify skin in the face for a number of consecutive frames.
\end{itemize}
\subsubsection{Identifying the skin tone}
There are two expected properties of the cluster corresponding to the skin. It is likely to contain the largest number of pixels, since we'd expect most of the face to be composed of skin. It is also likely to fall within the range of expected human skin tones.
The difficulty of this problem, however, is that neither of these properties are guaranteed to hold.
For example, the face may be dominated by hair which could become the most prominent colour in the image. Likewise, changes in illumination or the presence of specular reflection might cause the skin tone of the face to be outside the expected range.
\\ \\
By considering both of these properties together, as a heuristic, it becomes more probable that the skin tone is identified correctly. I assign a score to each cluster, $C$, based on the number of elements in the cluster, $|C|$, and its distance from the expected range of skin tones, $d_C$.
\begin{equation*}
    \mathrm{score}(C) = \alpha\cdot |C| - \beta\cdot d_C
\end{equation*}
The constants $\alpha$ and $\beta$ were selected experimentally. The skin tone is identified as the center of the cluster with the maximum score.
% This is shown to improve on simply selecting the largest cluster in Section \ref{section:skin_tone_detection}.

\subsubsection{Identifying skin pixels using the skin tone}
Identifying the skin tone, as described, requires the use of the k-means algorithm,
however it is not particularly suitable for real-time applications (see Section \ref{impl:kmeans_bad}). Thus, the skin tone cannot be identified in every frame.
Given the skin tone from an earlier frame, the value must be used to robustly detect skin pixels in subsequent frames. This is challenging since there will almost certainly be changing illumination conditions as well as changing positions of the skin pixels.
\\ \\
% A possible approach could be to record the number of pixels that were in the skin cluster when the skin tone was identified. Then, our algorithm could select the $n$ closest pixels in the colour space to the skin tone, where $n$ was the number of pixels in the skin cluster. However, this would not be robust to rotations of the face, since, in subsequent frames, the number of pixels visible to the camera could change. Assuming a static value of $n$ would not be robust.
% \\ \\
Implicitly, the clustering approach relies on the assumption that any change in illumination between frames affects all pixels equally. 
It assumes that in subsequent frames, the true skin pixels remain closer (in terms of Euclidean distance) to the skin tone than the non-skin pixels.
%That is, the pixels which were within the skin cluster when k-means was applied, are still closer to the skin tone than non-skin pixels in subsequent frames. 
This is a fairly strong assumption and is affected by phenomena such as shadows and specular reflection, which do not affect all pixels equally.
However, it is useful enough to make the problem more tractable than it was previously, without undermining its accuracy. %if the skin tone is detected frequently enough. 
%The validity of this assumption is made clear by the evaluation of the algorithm presented in Section \ref{section:skin_tone_detection}.
% Therefore, we proceed by classifying pixels based on their Euclidean distance from the skin tone.
% \\ \\
\paragraph{A Bayesian approach}
Recall that the first approach described classifies skin based on knowledge about the range of possible human skin tones. 
In this sense, it acts as a prior distribution; when classifying an individual pixel, it considers nothing of the face being presented.
The k-means implementation, on the other hand, considers the face presented but it has no knowledge of the prior. It instead assumes that the largest cluster must be the set of skin pixels.
\\ \\ 
A natural way to combine these two approaches would be to consider the problem of skin classification from a Bayesian perspective. Given some pixel $x_i$ we want to discover the likelihood of it being a skin pixel, having been conditionalised on the skin tone of the person as well the colour of the pixel being classified. 
We have access to the prior distribution from the first approach, which indicates the likelihood of a particular colour being skin.
Let us begin by denoting the following: 
\begin{itemize}
   \item $C_{\mathrm{skin}}$ the class of skin pixels
   \item $x_i$ the colour of the pixel being considered
   \item $s$ the skin tone of the face
\end{itemize} 
Given this notation the probability we wish to discover is the likelihood of a pixel being skin given its colour and the skin tone of the user.
\begin{equation*}
   \Pr(C_{\mathrm{skin}}| x_i, s) 
\end{equation*}
This can be re-written using Bayes' theorem to expose the prior distribution.
\begin{align*}
   \frac{\Pr(C_\mathrm{skin}, x_i, s)}{\Pr(x_i, s)} = \frac{\Pr(s|C_\mathrm{skin}, x_i)\Pr(C_\mathrm{skin}|x_i)\Pr(x_i)}{\Pr(x_i,s)}
\end{align*}
Once the distribution $\Pr(C_\mathrm{skin}|x_i,s)$ has been computed for a single frame, it is used as the prior for subsequent frames to provide resistance to 
sudden changes in the image.
% We could proceed with classification as follows, for some threshold probability $t$ that defines the decision boundary: 
% \begin{equation*}
%     \text{isSkin}(x_i) = 
%     \begin{cases}
%         \texttt{True}, \quad \text{if} \Pr(C_{\mathrm{skin}}| x_i, s) > t \\
%         \texttt{False}, \quad \text{otherwise}
%     \end{cases}
% \end{equation*}
% \begin{align*}
%     \Pr(C_\mathrm{skin}, x_i, s) &> \Pr(C_\mathrm{not-skin}, x_i, s) \\
%     \frac{\Pr(s|C_\mathrm{skin}, x_i)\Pr(C_\mathrm{skin}|x_i)\Pr(x_i)}{\Pr(x_i,s)} &> \frac{\Pr(s|C_\mathrm{not-skin}, x_i)\Pr(C_\mathrm{not-skin}|x_i)\Pr(x_i)}{\Pr(x_i,s)} \\
%     \Pr(s|C_\mathrm{skin}, x_i)\Pr(C_\mathrm{skin}|x_i) &> \Pr(s|C_\mathrm{not-skin}, x_i)\Pr(C_\mathrm{not-skin}|x_i) 
% \end{align*} 
% Since we're assuming that there are only two possible classes, this is equivalent to: 
% \begin{align*}
%     \Pr(s|C_\mathrm{skin}, x_i)\Pr(C_\mathrm{skin}|x_i) &> [1-\Pr(s|C_\mathrm{skin}, x_i)][1-\Pr(C_\mathrm{skin}|x_i)] \\
% \end{align*}
% So an equivalent condition in only a single class is: 
% \begin{equation*}
% \Pr(s|C_\mathrm{skin}, x_i)+\Pr(C_\mathrm{skin}|x_i) > 1
% \end{equation*}
% \Pr(s|C_\mathrm{skin}, x_i)\Pr(C_\mathrm{skin}|x_i) > t
\paragraph{Prior distribution}
Usually, one might attempt to learn the distributions $\Pr(C_\mathrm{skin}|x_i)$ and $\Pr(s|C_\mathrm{skin}, x_i)$ from a relevant dataset.
The prior $\Pr(C_\mathrm{skin}|x_i)$ is computed as the empirical distribution from 
the dataset discussed in Section \ref{section:colour-filter} which consists of randomly sampled pixels that are classified as skin or not. Since it does not include entire classified faces and, to the best of my knowledge, there are no publicly available, adequately sized datasets with this information learning the distribution $\Pr(s|C_\mathrm{skin}, x_i)$\footnote{denoted as the \textit{class conditional distribution}} is infeasible. As a result, it must be defined by hand.

\paragraph{Class conditional distribution}
A reasonable approach is to define the distribution $\Pr(s|C_\mathrm{skin}, x_i)$ based on the assumption that a skin pixel is likely close in colour to the overall skin tone. A large probability should be returned if the Euclidean distance (in the colour space) between the pixel and the skin tone is small.
There are, however, an infinite number of functions which could encode this. The only requirement is that the probability of being a skin pixel is decreasing as a function of the distance between the colour of the pixel and the skin tone of the face. 
\\\\
A reasonable assumption might be that if the pixel being considered, $x_i$, is a skin pixel, then the Euclidean distance between the skin tone, $s$, and the skin pixel, $d(s,x_i)$, varies as a Normal distribution with mean zero and standard deviation $\sigma$.
\begin{equation*}
   d(s, x_i) \sim N(0, \sigma) 
\end{equation*}
However, since colours are only represented in a finite interval, for example, between zero and one (or 0 and 255), this is not strictly correct. 
The above model has a non-zero probability associated with impossible colour values, since there are minimum and maximum possible distances.
As a result, I use a \textit{truncated} Normal distribution\footnote{A truncated distribution modifies the domain of a distribution by defining an updated probability density function which is zero outside a particular range $[a,b]$. This is achieved in such a way that the properties of a valid probability distribution are maintained.} instead.
\\\\
The probability density function of a truncated normal distribution between the values $a$ and $b$ and with mean, $\mu$, and variance, $\sigma$: 
\begin{equation*}
    f(x) = \frac{1}{\sigma}\frac{\phi(\frac{x-\mu}{\sigma})}{\Phi(\frac{b-\mu}{\sigma}) - \Phi(\frac{a-\mu}{\sigma})}
\end{equation*}\footnote{where $\phi$ and $\Phi$ are the probability density and cumulative distribution functions of a standard Normal distribution respectively.}
\noindent
In this case, the values of $a$ and $b$ represent the minimum and maximum distances respectively from the given skin tone $s$. 
Hence, $a=0$, since the pixel being considered may have the same colour as the skin tone, in which case, $d(x_i, s) = d(s, s) = 0$.
The value of $b$ here is the distance between the skin tone and the furthest possible point in the colour space.
In this approach, a three dimensional colour space is used and, hence, the value of $b$ is the Euclidean distance between $s$ and the furthest corner of the colour-space cube.
\\\\
Given this, the probability is defined in proportion to the probability density function of the truncated distribution for each possible skin tone.
\begin{equation*}
    \Pr(s|C_\mathrm{skin}, x_i) = \frac{f(d(s,x_i))}{\sum_{s' \in C}f(d(s',x_i))}
\end{equation*}
\paragraph{Avoiding the need for thresholding}
The best choice of the threshold value is not obvious, but in this context, classification is not the goal.
The aim of the algorithm is to minimise the effect of non-skin pixels on the mean, so instead I proceed by taking a weighted 
mean colour based on the probability of each pixel being a skin pixel. 

\paragraph{Accelerating class conditional computation}
Computing the class-conditional distribution for each pixel in the bounding box of the face is a costly computation. 
The distribution defined is continuous in nature and so maintaining floating point accuracy means the distribution cannot be precomputed (with finite memory).
Instead, by representing each component of the skin tone as an integer and rounding the Euclidean distance to the nearest integer, the distributions are precomputed 
for every possible skin tone. At runtime, the class conditional probability is achieved by lookup. Since the skin detection algorithm is applied to every frame in the video, 
using precomputed distributions achieves a 2x speedup in the overall pipeline (see Section \ref{eval:region_selection}).

\begin{figure}[H]
    \centering
    % Have an image from face to -> pixels in colour space on a graph with classification ->  to what classification looks like
    % \includegraphics[width=0.5\textwidth]{example-image-a}
    \subfloat[Original image]{{\includegraphics[width=4.9cm]{implementation/c.jpg} }}%
    \qquad
    \subfloat[Proposed]{{\includegraphics[width=5.2cm]{implementation/hmap-763-763.png} }}%

    \subfloat[Original image]{{\includegraphics[width=4.9cm]{implementation/c-2.jpg} }}%
    \qquad
    \subfloat[Proposed]{{\includegraphics[width=5.2cm]{implementation/hmap-678-678.png} }}%
    % Give an example image with prior etc.
    % \includegraphics[width=0.5\textwidth]{example-image-b}
    % \input{implementation/test.pgf}
   \caption{\textit{An example result from the described skin detection algorithm} }
   \label{fig:bayesian_ex}
\end{figure}
\noindent
The approach described achieves similar results to applying k-means per frame (see Figure \ref{fig:bayesian_ex}), but at an amortized cost 
inversely proportional to the number of frames in the video. In practice, this results in a order of magnitude speedup in the time to process a frame
for videos with more than a single frame (see Appendix). This means that skin detection is plausible whilst maintaining performance.


%Discrete or not??
% There is a finite range of possible values for the Euclidean distance between the colour of a pixel and the skin tone. This is because colours are discretised by the camera and therefore must lie between some set of bounds (usually 0 and 255). As a result, the defined distribution should also be discrete in nature.
% \\\\ 

 
% \begin{equation}
%    \Pr(C_{\mathrm{skin}}| x_i) = \Pr(C_{\mathrm{skin}}| x_i, \text{skin-tone} )\Pr(\text{skin-tone})
% \end{equation}


%I deem it to be a strong enough assumption to simply the problem adequately but not so strong as to undermine the effectiveness of the solution. 


% Describe how this addresses each of the problems

% Run multiple times with different k, and look for colours that repeat in each one to identify skin tone
% Use rudimentary filter to remove obviously wrong ones
% Consider cluster size

% TALK ABOUT COMPLEXITY OF K-MEANS AND WHY USING IT REPEATEDLY DOESN'T WORK THAT WELL
% Detect skin tone every x frames 
% Then define a probability distribution based on d

% Morphologies
% The rudimentary colour-filtering approach provides a good baseline as to the 










\section{Heart rate isolation}
% EXPLAIN WHY IT'S NOT JUST THE FOURIER POWER SPECTRUM
% IN VIDEOS WITH MOVEMENT WE EXPECT HEART RATE TO BE A SERIES OF PEAKS TOGETHER RATHER THAN A SINGLE PEAK
% THAT MIGHT BE DUE TO LIGHTING ETC
% IMPACT OF LIGHTING CONDITIONS
Given a time series of the mean colour in each frame, inferring the heart rate might, naÃ¯vely, be taken as the prevalent frequency. That is, the largest peak in the Fourier transform. 
However, it is important to consider that the colours observed are not only the result of the underlying biological phenomenon of interest. This naÃ¯ve assumption is prone to returning, instead,
 the frequency of some other factor that impacts colour of the face. For example, respiration or other movements of the face will have an impact, as well as any repetitive changes in lighting such as flickering. 
To simplify isolating the correct heart rate signal, the problem is broken down into two subtasks:
\begin{itemize}
    \item Identifying the pulse signal: given the noisy time series of observed colours for each frame, identify the signal corresponding to the pulse of the user.
    \item Identifying the heart rate: given the pulse signal, identify the heart rate.
\end{itemize}
Although, as will be explained, there is some overlap between the two to aid with correctly identifying the pulse.

\subsection{Blind-source separation}
\label{implementation:pca_ica}
Suppose that our observed colour signal, $\mathbf{I}(t)$, a vector-valued function, consists of a mixture of several underlying signals, $x_i(t)$ one of which is the pulse, $p(t)$. 
Our observed signal exists in three dimensions and can be viewed as the result of the mixing of three underlying signals by some mixing matrix $\mathbf{A}$.
\begin{equation*}
    \mathbf{I}(t) = \mathbf{A}\begin{bmatrix} x_1(t) \\ x_2(t) \\ x_3(t) \end{bmatrix}
\end{equation*}
% If we consider $\mathbf{I}(t)$ as a matrix of dimension $n \times T$ where $n$ is the number of colour dimensions and $T$ the number of timesteps, then we might suppose that $ (x_1(t), x_2(t), ..., p(t), ..., x_{n-1}(t))^T = AI$. That is 
% That is: $\mathbf{I}(t) = (x_1(t), x_2(t), x_3(t))$, for constants $\lambda_i$. 
% Alternatively we can consider this as finding the set of basis vectors $v_1, v_2, v_3$ such that $I(t) = (I(t)\cdot v_1)v_1 + (I(t) \cdot v_2)v_2 + (I(t) \cdot v_3)v_3$. The key problem is finding the basis $v_1, v_2, v_3$ such that $p(t) = (I(t)\cdot v_i))v_i$ for some $i$ and further identifying the value of $i$.
The nature of this task is to identify and return $p(t)$ from only $\mathbf{I}(t)$ and is known as the blind-source separation problem see (Section \ref{ref:bss_prep}).
The above formulation is not enough to isolate the signal $p(t)$ from $\mathbf{I}(t)$ and  so assumptions must be placed on the nature of each $x_i(t)$ and $p(t)$ in order 
to solve this.
% \\ \\ 
% We can reframe this as finding some basis for our vector space $I(t) = \sum_{i}^{n}I(t)\cdot(v_i)v_i$

\subsubsection{Independent component analysis (ICA)}
A possible assumption might be that each of the $x_i(t)$ are statistically independent. 
That is, informally, one cannot gain any information about one of these signals given another. 
This assumption encodes the notion that the pulse should be entirely independent from the other phenomena impacting the observed signal.
\\\\
For example, suppose that some of the $x_i(t)$ are the result of a physical phenomenon such as lighting conditions. In this case, it is intuitive to expect there to be no mutual information between the pulse and the other constituent signals. The ICA algorithm \cite{ica} attempts to identify these signals based on this assumption, by using non-Gaussianity as a proxy for statistical independence.
\\\\
% talk about how this doesn't tell us which one is p(t) 
Crucially, however, this approach returns the signals $x_1(t)$, $x_2(t)$ and $x_3(t)$ but gives no indication regarding which signal corresponds to the pulse. In fact, the formulation of the ICA algorithm is such that each $x_i(t)$ are returned in a random order. Additional work must be undertaken to identify the pulse from the returned signal. 
\\\\
I proceed by attempting to identify the heart rate in each of these signals independently. This results in returning three different heart rate values, each of which has an associated power.
This power value, which is the magnitude of the term in the Fourier transform associated with a given frequency, acts as a natural way of quantifying the importance of a particular frequency on the overall signal.
\begin{figure}[H]
    \centering
    \subfloat[\textit{Raw video signal}]{\scalebox{0.43}{\input{implementation/ica_0.pgf}}}
    \subfloat[\textit{ICA Component 1}]{\scalebox{0.43}{\input{implementation/ica_1.pgf}}}
    \subfloat[\textit{ICA Component 2}]{\scalebox{0.43}{\input{implementation/ica_2.pgf}}}
    \subfloat[\textit{ICA Component 3}]{\scalebox{0.43}{\input{implementation/ica_3.pgf}}}
\end{figure}
% In 
% Hence, I assume that the correct heart rate corresponds to whichever has the largest power.
%  This particular assumption is evaluated in Section \ref{section:ica_assumption}.
% For example, the pulse is likely to be the most periodic of these signals, in which case, we could identify the pulse as simply the 

% \subsubsection{Principal components analysis (PCA)}
% Alternatively, one might reframe the problem as finding a new orthogonal basis $\mathbf{v}_1, \mathbf{v}_2, \mathbf{v}_3$ to represent each vector $\mathbf{I}(t)$ in. The basis vectors are in the directions of maximal variance of the observed signal $\mathbf{I}(t)$. Finding these basis vectors is known as Principal Components Analysis.
% \\\\
% In this scenario, we assume that the pulse occurs in the direction of most variance.
% Under the assumption that $\mathbf{v}_1, \mathbf{v}_2, \mathbf{v}_3$ are in order from most to least variance, we return $p(t) = \mathbf{I}(t)\cdot\mathbf{v}_1$.
% Since we are only interested in the first basis vector, this is equivalent to fitting a plane to the three dimensional signal $\mathbf{I}(t)$ that minimises the squared error. The resulting pulse signal is the projection of $\mathbf{I}(t)$ onto this plane.
% ASSUMPTION OF CAMERA FEED BEING A MIXTURE OF INDEPENDENT SOURCES INCLUDING HEART RATE

% SELECTION OF RESULTING COMPONENT => HIGHEST PEAK

%SHOW IDEALISED HR AND WHY AUTOCORRELATION HELPS US PICK IT OUT


% \subsubsection{Principal components analysis}
% One might assume that each of the signals is 
% \subsection{Blind source separation}
%\subsection{Respiration rejection}

%\subsection{The sample-rate problem}
%\subsection{}


%talk about filtering stuff
% \paragraph{Conclusion}
% The evaluation of these two algorithms reflects whether the assumption of statistical independence or maximal variance better reflects the issue of isolating the heart rate. The performance of these two approaches are evaluated in Section \ref{section:bss}, from which it is concluded that ICA better serves the requirements of heart rate isolation. Hence, it is concluded that the assumption of statistical independence is more valid.
\subsection{Identifying the heart rate}
% given the pulse signal return HR
Given a pulse signal, the heart rate corresponds to the average number of beats of the heart (seen as spikes in the signal) in a minute. 
A possible approach to identifying this is to simply count the number of peaks in the signal. 
However, the Fourier transform provides a particularly rich representation from which to identify the heart rate and so is used instead. 
For example, the Fourier transform enables easy analysis of the relative powers of different peaks present in the power spectrum, which would not be easily possible with the former approach.
Crucially, it is not always the case that the most prevalent frequency is the true heart rate. There are two common cases that have been identified where this is often not true.

\subsubsection{The split peak issue}
%Talk about how heart rate can split over several peaks
%Main way to bypass this is to reward several close peaks together
%Hence the use of a butterworth filter
%Give EXAMPLE graphs
\begin{wrapfigure}{r}{0.5\textwidth}
    \centering
    \scalebox{0.6}{
    \input{implementation/butterworth.pgf}}
    \caption{\textit{An example of the split-peak issue}}
\end{wrapfigure}
In practice the true heart rate is not present in the resulting Fourier transform as a clear single peak. Instead, often, it will correspond to several peaks that are close by each with smaller powers. This phenomenon was reported by van der Kooij et al.~\cite{vanderKooij2019} and is denoted here as the \textit{split peak issue}.
In order to avoid this, a Butterworth filter is applied which removes isolated peaks and increases the power of peaks close together as is recommended by van der Kooij et al.~\cite{vanderKooij2019}.
% \begin{figure}[H]
%     % Add an image showing how in noisy Fourier transforms the HR is not just highest peak with result from Butterworth filter
%     \centering
%     \includegraphics[width=0.5\textwidth]{example-image-a}
%    \caption{An example of the split peak issue with a Butterworth filter applied} 
% \end{figure}
\subsubsection{Majority voting heuristic}
In scenarios where there are large amounts of movement in the video, the most prevalent frequency may not correspond to the heart rate. This is problematic, if the movement occurs at a frequency 
which could be otherwise interpreted as a heart rate, that is between 0.6Hz (36 beats per minute) and 4Hz (240 beats per minute). As a result, selecting the highest peak in this case could be 
incorrect. For these scenarios, I have developed a heuristic denoted here as the \textit{majority voting heuristic}.
\\\\
Given the three independent signals returned by ICA, we identify the largest peak in each signal. In most scenarios, these three heart rates will be relatively close together, but they tend not to be 
in scenarios with lots of movement, where a single large peak at a very different frequency is common. 
If this is the case, then we discard the frequency of maximum power and return the frequency with greatest power of the two remaining signals.
This helps to guard against a common pitfall of PPG-based systems where the frequency of movement is returned instead of the heart rate \cite{souza2019heart}.
 

\subsection{Summary}
% Here I have presented large aspects of original work which have combined to implement a system capable of rPPG
% Each of the relevant aspects are documented in the code that can be found below.
% I have presented a large body of work, of which much is original, that combines to implement a system capable of non-contact heart rate estimation. 
The work presented, largly, falls into two categories: improving the performance of the system and improving its fidelity. As two overarching goals
of the project, it is in this vein it will be evaluated.
All the described work is presented in the Python implementation and can be found in the supplied source code with documentation included.


\section{Repository overview}
\input{implementation/repo.tex}