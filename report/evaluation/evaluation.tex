This project seeks to increase access to heart rate sensing technology.
Since sensors, in general, behave in an online way, that is, they return measurements as and when they become available, the project should be expected to behave in the same way.
It should not rely on lengthy computations which a standard sensor would not be able to perform. 
Furthermore, the fidelity of its outputs should be in line with similar alternative sensors.
As a result, the evaluation proceeds by exemplifying the success of the project with respect to both of these criteria.

\section{Data collection}
In order to be able to evaluate the project, a dataset consisting of videos with an associated ground truth heart rate is required.
For this, I used the \textit{MAHNOB} dataset \cite{Mahnob}, initially collected by Imperial College London for research into affective computing, it is widely used in remote photoplethmysography literature for performance benchmarking \cite{novel}\cite{mahnob-example}\cite{mahnob-example-2}.
MAHNOB, however, only consists of videos where the participants are stationary and at a fixed, close distance to the camera (see Appendix for examples), this alone, would not be adeqeuate for testing many aspects of the project.
As a result, I augmented the dataset with my own experiments that contained varying amounts of movement and were recorded at different distances to the camera. 
Through this, the limitations of the project can be documented more effectively.
% This acts as a means of testing the boundaries and feasibility of remote heart rate sensing.

% face tracking. 
% Hence, it was augmented with several videos which I recorded with varying amounts and types of movement, as well as at different distances.
% In order to better understand the performance of face tracking, a test set of several videos with varying amounts of movement was used. Some of the videos were taken from the MAHNOB dataset \cite{Mahnob}, which forms one of the key sources of data for the evaluation of this project.
% Initially, generated as a dataset for study into affective computing, it consists of several thousand videos of participants as they are exposed to stimuli, with their ECG responses being recorded. A subset of videos 
% were randomly selected and were augmented with additional videso to form the test dataset for this project.
% \\\\
% Some of these videos contained strictly `translational' movement, that is, the size of the face of the user remains largely constant as they move from one side of the frame to the other.
% Another more challenging kind of movement, denoted here as `rotational', is where the user rotates their face so that different parts of the face are in a view in different frames.
% Intuitively, one would expect this to be more difficult for the face tracker to deal with, since points will come in and out of view regularly.
\subsection{Methodology}
% data is used to evaluate: effect of distance, movement, stationery, smartwatch
The data collected has been used for two distinct reasons: the justification of implementation decisions and the profiling of the overall success of the project.
Since this project regards the measurement of health data, it is critical to understand in what scenarios its outputs cannot be considered reliable.
With this in mind, experiments were designed to discover any fundamental limitations of this kind of technology. The data was collected with the following overarching questions in mind:
\begin{itemize}
    \item Can the results of remote heart rate sensing be trusted?
    \item To what extent does the user's distance from the camera affect the fidelity of the outputs?
    \item Does the user have to be stationary with respect to the camera for the results to be accurate?
    \item How does the project compare to a PPG sensor on a wearable device?
\end{itemize}

% The data collected will be used to evaluate several different aspects of the program.

\paragraph{Experimental setup}
% talk about testing different distances
% movement
% ecg as ground truth
% developed a logging app which records raw sensor values from a smart watch, used for comparison
% tested on myself
% 27 recordings each at 1min
% smartwatch and ecg used
% ecg used as a ground truth and is considered accurate enough for use in medical scenarios
% smartwatch used for comparison against rPPG as a sensor it is designed to compete with
% raw PPG signal taken from the smartwatch and is processed in the same way as rPPG
% recorded on Pixel 3a phone videos
In these additional experiments, videos were recorded of myself at distances of 1m, 1.5m and 2m. 
At each of these distances, three different activities were recorded, each for one minute with three repeats being conducted, for a total of 27 minutes worth of video and associated heart rate data.
A ground truth was taken in the form of a chest-based ECG sensor and a smartwatch was used for comparison with the video-based heart rate.
The data taken from the smartwatch was the raw PPG sensor values, that is, not the estimated heart rate. This is so that the same heart rate isolation algorithms can be applied to both the 
remote PPG signal and the wrist based PPG signal making for a fair comparison.
Video recordings were conducted on a Pixel 3A mobile device at a framerate of 30 frames per second.

The activities used are as follows with the latter two having been selected as to test both raised heart rates and significant amounts of movement, thereby providing a test case not present in the MAHNOB database.
These exercises were, thereby, selected to be representative of the kind of scenarios in which rPPG might be used.
\begin{itemize}
   \item Stationary for sixty seconds
   \item Jogging on the spot for forty seconds followed by twenty seconds of rest
   \item Star jumps for forty seconds followed by twenty seconds of rest
\end{itemize}

% As a result, I recorded videos
%  several experiments on myself whilst wearing an ECG sensor and a Fossil smartwatch.
% A lightweight application I developed records raw values from the wrist-based PPG sensor 

% \paragraph{Ethics}

% \paragraph{Dataset}


% \paragraph{Ethics}

\section{Analysis of performance}
\label{section:face_tracking}
The main technique introduced for improving runtime performance is \textit{face tracking}. It is not clear, initially, the extent of the performance improvement and what error, if any, it might have 
on heart rate measurements. In this section, it is shown to give a performance boost across all tested videos with a minimum speedup of 3x and a maximum of 8x, with no, statistically significant, detrimental effect on measurement accuracy.

\subsection{Face tracking}
Face tracking was proposed in Section \ref{section:face_tracking_impl} as an alternative to detecting the face in each frame independently.
From the implementation alone, it is unclear as to whether or not it is beneficial. To answer this, several separate aspects of the algorithm must be evaluated. 
Specifically, any described performance gains must be shown clearly as working across stationary scenarios and situations with movement of the face being tracked. 
Furthermore, it must be ensured that face tracking is not less accurate than simply repeatedly detecting the face in each frame. 
To evaluate these properties several metrics are defined and are measured across a variety of test videos.
\paragraph{Research questions}
\begin{itemize}
    \item Does face tracking provide a performance boost over face detection?
    \item Does face tracking have the same fidelity as face detection?
    \item Does face tracking maintain its accuracy under increasing motion?
    \item What value should the \texttt{threshold} take?\footnote{As defined in the pseudocode in Section \ref{section:face_tracking_impl} the \texttt{threshold} value defines when the face tracker redetects the face, specificially, it defines the percentage change in the size of the face tracked before redetection. }
\end{itemize}
% Three main questions: 
% -is it a performance gain?  (for all videos)
% -is it resistant to motion?
% -is it as accurate?
% -effect on HR accuracy?
\paragraph{Metrics}
Face detection is a binary classification task. As an algorithm, it defines a boundary within which all pixels are defined as belonging to a face or not.
As a result, if we consider face detection as a ground truth, the results of face tracking can be compared using standard classification metrics. In this case, I proceed by considering the following outcomes from the face tracker.
\begin{itemize}
   \item False negative (FN): pixel is incorrectly classified as not belonging to a face
   \item False positive (FP): pixel is incorrectly classified as belonging to a face
   \item True negative (TN): pixel is correctly classified as not belonging to a face 
   \item True positive (TP): pixel is correctly classified as belonging to a face 
\end{itemize}
These metrics can be combined to define the recall and precision of the output of the face tracker over a given frame. These respectively represent the rate at which true skin pixels are correctly identified as such and the likelihood that a skin prediction is correct.
\begin{equation*}
    \mathrm{recall} = \frac{\mathrm{TP}}{\mathrm{TP} + \mathrm{FN}} \text{ and } \mathrm{precision} = \frac{\mathrm{TP}}{\mathrm{TP}+\mathrm{FP}}
\end{equation*}
The face detector being used as a ground truth here, is in itself, imperfect. This is because, in general, face detection returns a bounding box
that will contain some pixels from the background of the image but contain the vast majority face pixels. In this sense, the face detector itself has almost perfect recall but imperfect precision.
Since the face tracker was designed, as described in Section \ref{section:face_tracking_impl}, to mimic face detection with better performance, recall is the most important metric. However, perfect recall
could be achieved by returning the entire frame as the bounding box of the face. As a result, the overall goal is to achieve perfect recall with a minimal false positive rate and so these are the metrics investigated.

% \begin{figure}[H]
%     \centering
%     \includegraphics[width=0.5\textwidth]{example-image-a}
%    \caption{A labelled diagram outlining the four metrics defined above.} 
% \end{figure}
% \noindent
% \paragraph{Computing the classification outcomes} The result of each face detection is represented as a matrix of zeros and ones over the size of the entire frame.
% Pixels within the bounding box of the face have value one and pixels outside have value zero in this matrix. Using this representation, for a given frame, the output from the face detector, $\mathbf{D}$ and the output of the face tracker $\mathbf{T}$, we compute the metrics as follows: 
% \begin{align*}
%     \mathrm{FP} &= H(\neg{\mathbf{D}} \wedge \mathbf{T}) \\
%     \mathrm{FN} &= H(\mathbf{D} \wedge \neg{\mathbf{T}})\\
%     \mathrm{TP} &= H(\mathbf{D} \wedge \mathbf{T})\\
%     \mathrm{TN} &= H(\neg{\mathbf{D}} \wedge \neg{\mathbf{T}})
% \end{align*}
% Under the following notation: 
% \begin{itemize}
%     \item The function $H(\mathbf{A})$ returns the number of ones present in the matrix $\mathbf{A}$
%     \item $\wedge$ is an elementwise, bitwise AND 
%     \item $\vee$ is an elementwise bitwise OR operation 
%     \item $\neg$ is a NOT operator on each element in the matrix
% \\\\
% \end{itemize}
Since face tracking is being proposed as an optimisation, the time to process each frame is also recorded as a means of measuring the relative performance of each algorithm.
\par
Finally, the entire test suite is run using both face detection and tracking to ensure there are no statistically significant increases or decreases in the error of the heart rate measurements.
% Furthermore, the signal to nois
% Finally, the distance moved by each set of points being tracked between frames is recorded. This metric acts a means of encoding the amount of movement in the video.


\paragraph{Robustness}
% The performance of the above algorithm, clearly depends on the proportion that each of the two branches are executed.
% Most important is not having false positives, so emphasis precision
% It returns all face pixels, with some number of background pixels included. 
% As a result, the cost of false negatives and false positives is not equal.  
% A false negative, that is, the face tracker has incorrectly classified a pixel as non-skin
% \begin{figure}[H]
%     \centering
%     \input{evaluation/recall.pgf}
% %    \caption{} 
% \end{figure}
% \paragraph{Motion resistance}
Lower threshold values, as expected, tend to result in better recall by the tracker, at the cost of more redetection. However, the extent to which this is true is not a constant across all videos.
Videos with more movement, cause a faster degradation in recall as the threshold is increased. Hence, a more conservative threshold value is favourable, especially since all recorded threshold values 
showed a perfromance boost.
\begin{table}[]
    \begin{tabular}{c|cc|cc|cc}
    \textbf{} & \multicolumn{2}{c|}{\textbf{Stationary}} & \multicolumn{2}{c|}{\textbf{Star jumps}} & \multicolumn{2}{c}{\textbf{Jogging}} \\
    \textbf{Threshold} & \textbf{Recall} & \textbf{FPR} & \textbf{Recall} & \textbf{FPR} & \textbf{Recall} & \textbf{FPR} \\ \hline
    \textbf{0.100} & 0.931237 & 0.000766 & 0.948175 & 0.002220 & 0.949240 & 0.000384 \\
    \textbf{0.125} & 0.925882 & 0.000764 & 0.943530 & 0.002673 & 0.928708 & 0.000507 \\
    \textbf{0.150} & 0.921940 & 0.000682 & 0.938508 & 0.003175 & 0.909149 & 0.000868 \\
    \textbf{0.175} & 0.920470 & 0.000696 & 0.936067 & 0.002992 & 0.894600 & 0.000742 \\
    \textbf{0.200} & 0.919102 & 0.000826 & 0.936017 & 0.003432 & 0.871856 & 0.002099 \\
    \textbf{0.250} & 0.921081 & 0.000775 & 0.937609 & 0.004346 & 0.846058 & 0.001295 \\
    \textbf{0.300} & 0.916228 & 0.000728 & 0.934811 & 0.005084 & 0.824485 & 0.001196 \\
    \textbf{0.350} & 0.911905 & 0.000648 & 0.932703 & 0.005788 & 0.816987 & 0.001551 \\
    \textbf{0.400} & 0.918874 & 0.000866 & 0.936631 & 0.007110 & 0.798890 & 0.001676 \\
    \textbf{0.450} & 0.918034 & 0.000810 & 0.932281 & 0.007136 & 0.800861 & 0.001595 \\
    \textbf{0.500} & 0.916427 & 0.001041 & 0.927878 & 0.008250 & 0.783104 & 0.003307
    \end{tabular}
    \caption{\textit{A table showing the recall and false positive rate (FPR) of the face tracker across the test suite and at with different threshold values}}
    \end{table}

\paragraph{Performance cost}
% talk about redetection rates
Recall from Section \ref{section:face_tracking_impl} that the following inequality was introduced that describes the performance of face tracking.\footnote{Notation: $R$: number of redetections, $W$: number of frames considered, $f(n)$: time to detect a face in a frame of size $n$, $g(p,n)$: time to track $p$ points on a face of size $n$, $s(p,f)$: time to select $p$ points on a face of size $f$}
\begin{equation*}
    \frac{R}{W} < \frac{f(n)-g(p,n)}{f(n)+s(p,f)}
\end{equation*}
The above inequality relates the rate of redetections ($R/W$) and the costs of several required algorithms. It was shown in Section \ref{section:face_tracking_impl} that when this inequality holds, face tracking provides a performance benefit.
In order to justify this, the value of the right hand side is evaluated experimentally on a variety of videso and is shown to exceed the value $R/W$ across all videos and thresholds tested, as shown below.
\begin{figure}[H]
    \centering
    \scalebox{0.8}{\input{evaluation/redect.pgf}}
   \caption{\textit{The redection rate, $R/W$, computed over a number of different classes of videos. The theoretical upper bound is computed with a confidence interval shown.}} 
\end{figure}
As is expected, as the threshold increases the number of redetections, decreases. However, crucially, across all the thresholds tested, the redection rate is beneath the upper bound specified.

% plot redetection rates and max value

% plot mean time per frame with std

% \paragraph{Thresholds}
% show how threshold affects time and precision

\paragraph{Results}
Across all the videos tested and all the threshold values, face tracking is shown to give performance benefits over face detection.
Ranging from an average of 8x in the stationary case, to 3x in exercise videos.
\begin{figure}
    \centering
    \scalebox{0.8}{\input{evaluation/tracking_time.pgf}}
   \caption{\textit{The mean time (with confidence intervals) to a process a single frame is shown for both face detection and face tracking.}} 
\end{figure}
\noindent
Using a null hypothesis that both face tracking and face detection have an identical expected error, an independent two sample t-test, p-values over each type of video tested are reported.
Given a significance threshold of 0.1, the null hypothesis cannot be rejected for both stationary and videos with jogging.
It is in fact rejected for the case of star jumps, but in favour of the the proposed face tracker.
On these grounds, I deem that it would be unreasonable to suggest that face tracking, as 
opposed to face detection, introduces any unexpected error in the outputs of the program.
% \begin{figure}[H]
%     \centering
%     \input{evaluation/tracker_err.pgf}
%    \caption{\textit{Using face tracking with a threshold of 0.3, the percentage error with face detection and face tracking is shown.}} 
% \end{figure}
\begin{table}
    \begin{tabular}{c|cc|cc|cc}
    \multicolumn{1}{l|}{} & \multicolumn{2}{c}{\textbf{OpenCV}} & \multicolumn{2}{c|}{\textbf{Proposed}} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{} \\
    \textbf{Type of video} & \textbf{MAE (bpm)} & \textbf{std.} & \textbf{MAE (bpm)} & \textbf{std.} & \textbf{t-statistic} & \textbf{p} \\ \hline
    Stationary & 9.007 & 9.309 & 7.959 & 9.391 & 2.172 & 0.030 \\
    Jogging & 65.551 & 15.146 & 64.128 & 16.655 & 1.732 & 0.084 \\
    \rowcolor[HTML]{EFEFEF} 
    Star jumps & 20.035 & 15.314 & 19.752 & 14.989 & 0.366 & 0.714 \\ \hline
    \end{tabular}
    \caption{\textit{A table reporting the mean absolute error (MAE) in estimating the heart rate when using the OpenCV face detector on each frame vs the proposed face tracker, with independent two-sample t-tests shown. }}
    \end{table}

% show graph of overall error


\paragraph{Summary}
% Provides a huge performance benefit with little cost
% Drives up the possible frame rate and hence sampling rate of the virtual sensor
% Improves performance on all tested videos and at all tested thresholds with no significant change in accuracy of fidelity

Face tracking, as described, provides large performance benefits across all videos tested.
Critically, this comes without any evidence of a detrimental effect on fidelity. As an optimisation, this
is important since, although the videos in the test suite have a fixed frame rate, which would be exceeded by face detection, it allows future work to 
use higher frame rate cameras whilst still maintaining real-time behaviour. 
% \paragraph{Correctness}

% \section{Region selection}
% \label{section:region_selection}
% The problem of region selection does not, at an initial glance, lend itself to easy evaluation.
% It is unclear, through reasoning exclusively, whether increasingly complex algorithms for skin detection provide any benefit to the overall accuracy of the heart rate prediction.
% % In fact, it is only hypothesised that any form of filtering on the bounding box 
% In Section \ref{ref:region_selection_impl} it was weakly hypothesised that minimising the number of non-skin pixels included in the mean colour would decrease the noise in the resulting signal. 
% This was made under the further assumption that a less noisy signal would result in more accurate heart rate predictions. 
% Underpinning these assumptions is the notion that any pixels in the background, that is within the bounding box of the face but not true skin pixels, will contain no predictive power regarding the heart rate of the user.
% \\\\
% Crucially, however, it is unclear as to whether or not there exists some tradeoff between the number of pixels considered and their individual fidelities. 
% It could be reasonably argued that by considering more pixels, the resulting average becomes more resistant to noise at individual pixel level, say that caused by fluctuations in lighting, for example.
% It is not immediately clear whether considering a small set of guaranteed skin pixels is better than considering a large number of noisy pixels.
% \\\\
% This evaluation, hence, is concerned with the validation of these assumptions.
% \subsection{Skin tone detection}
% \label{section:skin_tone_detection}
% % Evaluate using the primitive vs basic skin tone range vs k-means vs bayesian 
% % Show improvement of scoring cluster centers vs just picking the largest cluster
% \paragraph{Research questions}
% % is considering a subset of pixels even beneficial?
% % what effect does it have on SNR?
% % how does that affect overall accuracy?
% \begin{itemize}
%    \item Is considering a subset of pixels within the bounding box advantageous? 
%    \item Does skin detection reduce the noise of the resulting colour signal?
%    \item Does a less noisy signal improve accuracy of heart rate predictions?
% \end{itemize}

% \paragraph{Metrics}
% %-effect of each on signal to noise ratio over the entirety of MAHNOB
% %-effect on accuracy of overall hr prediction
% %-time cost


% \paragraph{Results}

% \paragraph{Summary}
% % \subsection{}
% % \paragraph{Research questions}
% % % doe
% % \paragraph{Metrics}
% % \paragraph{Results}


% \section{Heart rate isolation}
% \subsection{Identifying the pulse signal}
% \label{section:bss}
% \paragraph{Research question}
% % ICA vs PCA
% \begin{itemize}
%     \item Which algorithm for blind source separation performs better for identifying the pulse signal?
% \end{itemize}
% \paragraph{Metrics}
% \paragraph{Results}
% %The PCA approach comes with the benefit of identifying the pulse signal as a natural part of the algorithm.
% \paragraph{Summary}

% \subsection{Extracting the heart rate from the pulse signal}
% \subsubsection{Independent components analysis}
% \label{section:ica_assumption}
% \paragraph{Research question}
% \begin{itemize}
%     \item How often does the frequency of maximum power correspond to the true heart rate?
% \end{itemize}
% % evaluate correctness of assumption of maximum power from ICA applied to each signal
% \paragraph{Metrics}
% \paragraph{Results}

% \paragraph{Summary}

% \subsubsection{Principal components analysis}
% \paragraph{Research question}
% % does component with most variance actually correspond to the pulse
% \begin{itemize}
%     \item How correct is the assumption that the signal of maximum variance corresponds to the pulse?
% \end{itemize}
% \paragraph{Metrics}
% \paragraph{Results}
% \paragraph{Summary}

% EXTRA JUST IN CASE
% Not that important?
% \subsection{Window and stride size}
% \paragraph{Research question}
% \paragraph{Metrics}
% \paragraph{Results}
% \paragraph{Summary}


\section{Analysis of sensing fidelity}
% \section{Performance}
% \subsection{Accuracy}
% cover stationary and exercise videos
% Performance, without sensing fidelity is entirely irrelevant. A sensor wit
The fidelity of a sensor forms the key basis upon which it is evaluated. It is critical to be able to reason about how trustworthy its outputs are and, more importantly, to understand
when its results cannot be trusted. To such end, evaluating the project, in this vein, is paramount. I proceed by defining metrics to quantify the fidelity of my rPPG implementation and end by comparing to a wrist-based PPG sensor.

\paragraph{Research questions}
% essentially this is where we evaluate vs a smartwatch
% two questions: 
% -is RPPG viable for stationary videos?
% -is it better than a smartwatch?
% NEED TO TALK ABOUT INCREASED COMPUTATIONAL AVAILABILITY FOR A PHONE OVER SMARTWATCH
% But also greater difficulty of sensing
% need to say that more data must be collected before production but initial results are promising

% likely to be that rPPG degrades with distance but smartwatches obviously don't
% certain classes of exercise, where face is stationary but arm is moving is probably better for rPPG
\begin{itemize}
    \item Is remote heart rate sensing viable for stationary videos?
    \item To what extent does accuracy depend on distance and movement?
    \item Can it replicate or exceed the performance of a wearable device?
\end{itemize}

% \subsection{Stationary videos}
% \subsection{The effect of movement}
% \subsection{Comparative fidelity}
\paragraph{Metrics}
\begin{itemize}
    \item 
    \item 
\end{itemize}
\paragraph{Results}

\paragraph{Summary}
% these ones are EXTRAS just in case
% \subsection{Energy consumption}
% \paragraph{Research question}
% \paragraph{Metrics}
% \paragraph{Results}

% \subsection{Time cost}
% \paragraph{Research question}
% \paragraph{Metrics}
% \paragraph{Results}
\begin{figure}
    \centering
    \scalebox{0.8}{\input{evaluation/fidelity.pgf}}
   \caption{\textit{}} 
\end{figure}

\section{Summary of findings}
% Face tracking provides large performance benefit without which 'real-time' performance would not be possible
% Allows for Bayesian Skin Detection to be plausible
% rPPG not feasible for all scenarios depends on the amount and extent of movement
% it's not much less suitable than smart watch critically 